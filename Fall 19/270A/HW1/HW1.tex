\documentclass[11pt,letterpaper]{report}
\usepackage{amssymb,amsfonts,color,graphicx,amsmath,enumerate}
\usepackage{tikz}
\usepackage{amsthm}
\usepackage{bbm}

\newcommand{\naturals}{\mathbb{N}}
\newcommand{\integers}{\mathbb{Z}}
\newcommand{\complex}{\mathbb{C}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\exreals}{\overline{\mathbb{R}}}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\mable}{measurable}
\newcommand{\quats}{\mathbb{H}}
\newcommand{\rationals}{\mathbb{Q}}
\newcommand{\norm}{\trianglelefteq}
\newcommand{\Aut}{\text{Aut}}
\newcommand{\disk}{\mathbb{D}}
\newcommand{\halfplane}{\mathbb{H}}
\newcommand{\Lp}[2]{\left\|{#1}\right\|_{L^{#2}}}
\newcommand{\supp}[1]{\text{supp}({#1})}
\newcommand{\Hom}[2]{\text{Hom}_{{#1}}({#2})}
\newcommand{\tr}{\text{tr}}
\newcommand{\field}[1]{\mathbb{F}_{{#1}}}
\newcommand{\Gal}[1]{\text{Gal}\left({#1}\right)}
\newcommand{\esssup}{\text{ess sup }}
\newcommand{\essinf}{\text{ess inf }}
\newcommand{\affine}{\mathbb{A}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\ind}{\mathbbm{1}}
\newcommand{\Var}{\text{Var}}

\newenvironment{solution}
{\begin{proof}[Solution]}
{\end{proof}}

\voffset=-3cm
\hoffset=-2.25cm
\textheight=24cm
\textwidth=17.25cm
\addtolength{\jot}{8pt}
\linespread{1.3}

\begin{document}
\noindent{\em Liam Hardiman\hfill{October 28, 2019} }
\begin{center}
{\bf \Large 270A - Homework 1}
\vspace{0.2cm}
\hrule
\end{center}

\begin{enumerate}
	\item \begin{enumerate}
		\item Let $\mcal{F}$ be the family of all finite subsets of $\Omega$ and their complements. Is $\mcal{F}$ a $\sigma$-algebra?
		\begin{solution}
			If $\Omega$ is finite then $\mcal{F}$ is simply the power set of $\Omega$, which is definitely a $\sigma$-algebra. However, if $\mcal{F}$ is infinite, then $\mcal{F}$ is never a $\sigma$-algebra. To see this, let $(x_n)$ be a countable sequence of distinct elements in $\Omega$ and consider the set of even-indexed terms
			\[
				F = \{x_n: n = 2k,\ k\in \naturals\}.
			\]
			This set is a countable union of singletons and all singletons belong to $\mcal{F}$. $F$ is clearly infinite, but so is its complement, which contains the (infinite) set of odd-indexed terms. We conclude that $F$ is neither finite nor co-finite, so $\mcal{F}$ is not closed under countable unions when $\Omega$ is an infinite set.
		\end{solution}

		\item Let $\mcal{F}$ be the family of all countable subsets of $\Omega$ and their complements. Is $\mcal{F}$ a $\sigma$-algebra?
		\begin{solution}
			$\mcal{F}$ is indeed a $\sigma$-algebra. The empty set is clearly countable, and $\Omega^C = \emptyset$. Let $F_n$ be a countable collection of sets in $\mcal{F}$ and consider their union, $F = \cup_{n=1}^\infty F_n$. If each $F_n$ is countable, then $F$ is just a countable union of countable sets: countable. If one of the $F_n$'s, say $F_k$, were co-countable, then $F^C \subseteq F_k^C$, which is countable, so $F$ is co-countable. Since $\mcal{F}$ contains the empty set and $\Omega$ and is closed under countable unions and complements, it is a $\sigma$-algebra.
		\end{solution}

		\item Let $\mcal{F}$ and $\mcal{G}$ be two $\sigma$-algebras of subsets of $\Omega$. Is $\mcal{F}\cap \mcal{G}$ always a $\sigma$-algebra?
		\begin{solution}
			$\mcal{F}$ is a $\sigma$-algebra. Since $\mcal{F}$ and $\mcal{G}$ both contain $\emptyset$ and $\Omega$, so does their intersection. Let $E_n$ be a countable collection of sets in $\mcal{F}\cap \mcal{G}$. Since $\mcal{F}$ and $\mcal{G}$ are both $\sigma$-algebras, the union $E = \cup_{n=1}^\infty E_n$ is in both $\mcal{F}$ and $\mcal{G}$ and each $E_n^C$ is in both $\mcal{F}$ and $\mcal{G}$ as well.
		\end{solution}

		\item Let $\mcal{F}$ and $\mcal{G}$ be two $\sigma$-algebras of subsets of $\Omega$. Is $\mcal{F}\cup \mcal{G}$ always a $\sigma$-algebra?
		\begin{solution}
			The union need not be a $\sigma$-algebra. Let $\Omega = \{1, 2, 3, 4\}$, $\mcal{F} = \{\emptyset, \Omega, \{1\}, \{2, 3, 4\}\}$, and $\mcal{G} = \{\emptyset, \Omega, \{2\}, \{1, 3, 4\}\}$. $\mcal{F}$ and $\mcal{G}$ are $\sigma$-algebras, but the set $\{1\}\cup \{2\} = \{1,2\}$ is not in their union.
		\end{solution}
	\end{enumerate}

	\item A subset $A\subset \naturals$ is said to have asymptotic density if
	\[
	\lim_{n\to \infty}\frac{|A\cap \{1, \ldots, n\}|}{n}
	\]
	exists. Let $\mcal{F}$ be the collection of subsets of $\naturals$ for which the asymptotic density exists. Is $\mcal{F}$ a $\sigma$-algebra?
	\begin{solution}
		$\mcal{F}$ is not a $\sigma$-algebra. First let's construct a set not in $\mcal{F}$. The idea is to build a set that has long gaps followed by even longer ``runs''. Let $F_0 = \{1\}$ and $F_i = \{2^i, \ldots, 2^{i+1}-1\}$. Define the set $A$ by $A = \cup_{j=0}^\infty F_{2j}$. $A$ consists of a run of length $2^{2j}$ followed by a gap of length $2^{2j+1}$ for each $j = 0, 1, \ldots$. Our set $A$ does not have asymptotic density since
		\begin{align*}
			\frac{|A\cap [2^{2k}]|}{2^{2k}} &= \frac{\sum_{j=0}^{k-1}2^{2j}+1}{2^{2k}}\\
			&= \frac{1}{3}
		\end{align*}
		while on the other hand,
		\begin{align*}
			\frac{|A\cap [2^{2k+1}]|}{2^{2k+1}} &= \frac{\sum_{j=0}^{k}2^{2j}}{2^{2k+1}}\\
			&= \frac{1}{3}\left(2 - \frac{1}{2^{2k+1}}\right)\\
			&\to \frac{2}{3}.
		\end{align*}
		Hence, $A$ is not in $\mcal{F}$. Since $A$ is a countable union of singletons, which clearly have asymptotic density zero, we conclude that $\mcal{F}$ is not a $\sigma$-algebra.\\
	\end{solution}

	\item Let $X$ and $Y$ be two random variables on the same probability space $(\Omega, \mcal{F}, \Prob)$, and let $E\in \mcal{F}$ be an event. Define
	\[
	Z = \begin{cases}
		X&\text{if }E\text{ occurs}\\
		Y&\text{otherwise.}
	\end{cases}
	\]
	Prove that $Z$ is a random variable.
	\begin{proof}
		We can write $Z = X\cdot \ind_E + Y\cdot \ind_{E^c}$. Since $E$ is an event, the indicator functions $\ind_E$ and $\ind_{E^c}$ are measurable. Since $X$ and $Y$ are measurable and products and sums of measurable functions are measurable, we have that $Z$ is measurable, and hence a random variable.
	\end{proof}

	\item Let $X$ be a random variable with density $f$. Compute the density of $X^2$.
	\begin{solution}
		First let's compute the distribution of $X^2$. Let $t\geq 0$.
		\begin{align*}
			\Prob[X^2<t] &= \Prob[-\sqrt{t}<X<\sqrt{t}]\\
			&= \int_{-\sqrt{t}}^{\sqrt{t}}f(s)\ ds.
		\end{align*}
		By the Lebesgue differentiation theorem, the above integral is an almost everywhere differentiable function of $t$ and we can apply the fundamental theorem of calculus. If we let $g$ be the density of $X^2$ then
		\begin{align*}
			g(t) &= \frac{d}{dt}\int_{-\sqrt{t}}^{\sqrt{t}}f(s)\ ds\\
			&= \frac{1}{2\sqrt{t}}[f(\sqrt{t}) + f(-\sqrt{t})],
		\end{align*}
		for $t\geq 0$. Since $X^2$ is clearly nonnegative, we then have
		\[
		g(t) = \begin{cases}
			\frac{1}{2\sqrt{t}}[f(\sqrt{t})+f(-\sqrt{t})],&t>0\\
			0,&t\leq 0.
		\end{cases}
		\]
	\end{solution}

	\item Let $X$ be a nonnegative random variable. Show that
	\[
	\E[X] = \int_0^\infty \Prob[X>t]\ dt.
	\]
	\begin{proof}
		Since $X$ is nonnegative (this is important -- the Lebesgue integral is orientation-independent, unlike the Riemann integral!),
		\begin{align*}
			X &= \int_0^Xdt.
		\end{align*}
		We can then take the expectation of both sides and apply Fubini's theorem.
		\begin{align*}
			\E[X] &= \int_\Omega\int_0^Xdt\ d\Prob\\
			&= \int_\Omega\int_0^\infty \ind_{X>t}(t)\ dt\ d\Prob\\
	 		&= \int_0^\infty \int_\Omega\ind_{X>t}(x)\ d\Prob\ dt\\
			&= \int_0^\infty \Prob[X>t]\ dt.
		\end{align*}
	\end{proof}

	\item Let $\varphi:\reals\to \reals$ be a strictly convex function. Let $X$ be a random variable such that $\E[|X|]<\infty$ and $\E[|\varphi(X)|]\leq \infty$. Show that
	\[
	\varphi(\E[X]) = \E[\varphi(X)] \implies X = \E[X]\text{ a.s.}
	\]
	\begin{proof}
		Since $\varphi$ is strictly convex, for every $t\in \reals$ there exists an affine linear function $F_t(x)$ such that $F_t(t) = \varphi(t)$ and $F_t(x)<\varphi(x)$ for all $x\neq t$. We can set $t = \E[X]$ compose with $X$ to obtain $F_t(X)\leq\varphi(X)$, with equality if and only if $X = \E[X]$. Note that since $F_t$ is affine linear we have that $\E[F_t(X)] = F_t(\E[X])$.\\

		\noindent Suppose that $\varphi(\E[X]) = \E[\varphi(X)]$. When $t = \E[X]$, $F_t$ and $\varphi$ agree at $\E[X]$, so $\varphi(\E[X]) = F_t(\E[X])$, yielding
		\begin{align*}
			\E[\varphi(X)] &= \varphi(\E[X])\\
			&= F_t(\E[X])\\
			&= \E[F_t(X)].
		\end{align*}
		By the linearity of expectation we then have $\E[\varphi(X) - F_t(X)] = 0$. By convexity, $\varphi(X) - F_t(X)\geq 0$, so since this expectation is zero, we must have that $\varphi(X) = F_t(X)$ almost surely. By strict convexity, this implies that $X = t = \E[X]$ almost surely.
	\end{proof}

	\item Suppose $0\leq p_n\leq 1$ and put $\alpha_n = \min(p_n, 1-p_n)$. Show that if $\sum_n\alpha_n$ diverges, then no discrete probability space can contain independent events $A_1, A_2, \ldots$ such that $\Prob[A_n] = p_n$.
	\begin{proof}
		First let's strengthen the Borel-Cantelli lemma by proving a partial converse. Consider the tail $\cup_{n=M}^\infty A_n$ for $M$ large. We compute the probability of the tail's complement using the fact that $1-x\leq e^{-x}$.
		\begin{align*}
			\Prob[\cap_{n=M}^NA_n^c] &= \prod_{n=M}^N(1-\Prob[A_n])\\
			&\leq \exp\left(-\sum_{n=M}^N\Prob[A_n]\right)\\
			&\to 0\quad\text{as }N\to \infty.
		\end{align*}
		Since the complement of the tail goes to zero in probability, we have that $\Prob[\cup_{n=M}^\infty A_n] = 1$ for all $M$. Since $\limsup A_n = \cap_{M=1}^\infty \cup_{n=M}^\infty A_n$, continuity of measure tells us that $\Prob[\limsup A_n] = 1$.\\

		\noindent Suppose that these events live in a discrete probability space $\Omega$ (equipped with the power set $\sigma$-algebra). Since $\Omega$ is discrete, there must be some $\omega\in \Omega$ with $\Prob[\{\omega\}] > 0$. Define the sequence of events $E_n$ by
		\[
		E_n = \begin{cases}
			A_n^c,&\text{if }\omega\in A_n\\
			A_n,&\text{if }\omega \notin A_n.
		\end{cases}
		\]
		In particular, each $E_n$ misses $\omega$ and the $E_n$'s are independent. We also have that
		\[
		\sum \Prob[E_n] \geq \sum\alpha_n = \infty.
		\]
		By our strengthened Borel-Cantelli lemma, $\Prob[\limsup E_n] = 1$. By discreteness, we must then have that $\Omega = \limsup E_n$. But $\omega$, which has positive probability, isn't in $\limsup E_n$ -- a contradiction. We conclude that $\Omega$ is not discrete.
	\end{proof}

	\item Prove that if random variables $X$ and $Y$ are independent, then so are $f(X)$ and $g(Y)$, for any Borel measurable functions $f,g:\reals\to \reals$.
	\begin{proof}
		We need to show that for any Borel sets $B_1, B_2\in \mcal{B}(\reals)$, the events $(f\circ X)^{-1}[B_1]$ and $(g\circ Y)^{-1}[B_2]$ are independent. We can rewrite these preimages as
		\[
		(f\circ X)^{-1}[B_1] = X^{-1}[f^{-1}[B_1]],\quad (g\circ Y)^{-1}[B_2] = Y^{-1}[g^{-1}[B_2]].
		\]
		Since $f$ and $g$ are measurable, the preimages $f^{-1}[B_1]$ and $g^{-1}[B_2]$ are Borel sets. Similarly, since $X$ and $Y$ are random variables, the preimages $X^{-1}[f^{-1}[B_1]]$ and $Y^{-1}[g^{-1}[B_2]]$ are events. Since $X$ and $Y$ are independent, any preimage under $X$ is independent of any preimage under $Y$.
	\end{proof}

	\item Let $p\geq 3$ be prime. Let $X$ and $Y$ be independent random variables that are uniformly distributed on $\{0, \ldots, p-1\}$. Define
	\[
	Z_n = (X+nY)\pmod{p},\quad n = 0, \ldots, p-1.
	\]
	Show that the random variables $Z_n$ are pairwise independent, but not jointly independent.
	\begin{proof}
		First, we claim it suffices to prove that for all $s,t$ in $S = \{0, \ldots, p-1\}$ we have
		\begin{equation}\label{sufficient}
		\Prob[Z_{n_1} = s,\ Z_{n_2} = t] = \Prob[Z_{n_1} = s]\cdot \Prob[Z_{n_2} = t].
		\end{equation}
		Define $\mcal{A}_1$ and $\mcal{A}_2$ as follows.
		\[
		\mcal{A}_i = \{\Omega\}\cup \big\{\{Z_{n_i} = s\}: s\in S\big\}.
		\]
		$\mcal{A}_i$ is a $\pi$-system that contains $\Omega$. Since the singletons generate the power set $\sigma$-algebra, we have that $\sigma(\mcal{A}_i) = \sigma(Z_{n_i})$. Since (\ref{sufficient}) says that $\mcal{A}_1$ and $\mcal{A}_2$ are independent, we have that $Z_{n_1}$ and $Z_{n_2}$ are independent by the $\pi-\lambda$ theorem.\\

		\noindent Now let's actually verify (\ref{sufficient}).
		Let's start with the right-hand side. Since $X$ and $Y$ are independent, we have
		\begin{align*}
			\Prob[Z_{n_1} = s] &= \Prob[X = s-n_1Y]\\
			&= \sum_{y=0}^{p-1}\Prob[X=s-n_1y,\ Y=y]\\
			&= \frac{1}{p}\sum_{y=0}^{p-1}\Prob[X = s-n_1y]\\
			&= \frac{1}{p}.
		\end{align*}
		The right-hand side of (\ref{sufficient}) is then $\frac{1}{p^2}$.
		Now for the joint. If we assume that $n_1\neq n_2$ then the system
		\[
		\begin{array}{lllll}
			X &+& n_1Y &=& s\\
			X &+& n_2Y &=& t\\
		\end{array}
		\]
		has a unique solution $(\alpha, \beta)$ for $X,Y$ since $\integers/p\integers$ is a field. Sine $X$ and $Y$ are independent, the joint probability becomes
		\begin{align*}
			\Prob[Z_{n_1}=s,\ Z_{n_2}=t]&= \Prob[X = \alpha,\ Y = \beta]\\
			&= \Prob[X = \alpha]\cdot \Prob[Y = \beta]\\
			&= \frac{1}{p^2}.
		\end{align*}
		We have then verified (\ref{sufficient}), so the $Z_{n_i}$'s are pairwise independent.\\

		\noindent To see that the $Z_{n_i}$'s are not jointly independent, consider three variables $Z_{n_1}$, $Z_{n_2}$, $Z_{n_3}$, with distinct $n_i$. Such a trio exists since we've assumed $p\geq 3$. The joint event $\Prob[Z_{n_1}= r,\ Z_{n_2} = s,\ Z_{n_3} = t]$ represents the overdetermined system
		\[
		\begin{array}{lllll}
			X &+& n_1Y &=& r\\
			X &+& n_2Y &=& s\\
			X &+& n_3Y &=& t\\
		\end{array}.
		\]
		For any $[r\ s\ t]^T$ not in the span of $[1\ 1\ 1]^T$ and $[n_1\ n_2\ n_3]^T$, this system will have no solutions and $\Prob[Z_{n_1}=r,\ Z_{n_2}=s,\ Z_{n_3}=t] = 0$. However, $\Prob[Z_{n_1}=r]\cdot\Prob[Z_{n_2}=s]\cdot \Prob[Z_{n_3}=t] = \frac{1}{p^3}$. We conclude that the $Z_{n_i}$'s are not jointly independent.
	\end{proof}

	\item \begin{enumerate}
		\item For any given $\mu\in\reals$, $\sigma>0$, $k\geq 1$, show that there exists a random variable $X$ with mean $\mu$ and variance $\sigma^2$ for which Chebyshev's inequality becomes an equality:
		\[
		\Prob[|X-\mu|\geq k\sigma] = \frac{1}{k^2}.
		\]
		\begin{proof}
			Let's construct a random variable $X$ that takes values in $\{-k\sigma, 0, k\sigma\}$. To make things simple, let's construct $X$ to have zero mean (we'll shift it to $\mu$ later). In order for this to work, we need $\Prob[X = -k\sigma] = \Prob[X = k\sigma] = \beta$ and $\Prob[X = 0] = \alpha$ for some nonnegative $\alpha,\beta$ with $\alpha+2\beta = 1$. The variance of $X$ will then be given by
			\begin{align*}
				\Var[X] &= \E[X^2] = 2\beta k^2\sigma^2.
			\end{align*}
			In order for $\Var[X] = \sigma^2$ to hold, we need $\beta = \frac{1}{2k^2}$. Consider then the variable $X$ with
			\[
			X = \begin{cases}
				-k\sigma,&\text{with probability }\frac{1}{2k^2}\\
				0,&\text{with probability }1-\frac{1}{k^2}\\
				k\sigma,&\text{with probability }\frac{1}{2k^2}
			\end{cases}.
			\]
			We then have
			\begin{align*}
				\Prob[|X|\geq k\sigma] &= \Prob[X = \pm k\sigma]\\
				&= \frac{1}{k^2}.
			\end{align*}
			By linearity of expectation, the variable $X + \mu$ will have mean $\mu$, variance $\sigma^2$ and the above equality will still hold.
		\end{proof}

		\item Show that for any random variable $X$ with mean $\mu$ and variance $\sigma^2$, one has
		\[
		\Prob[|X-\mu|\geq k\sigma] = o\left(\frac{1}{k^2}\right)\text{ as }k\to \infty.
		\]
		\begin{proof}
			By Markov's inequality we have
			\begin{equation}\label{mark}
			\begin{split}
				k^2\cdot \Prob[|X-\mu|\geq k\sigma] &= k^2\cdot \Prob[(X-\mu)^2\geq k^2\sigma^2]\\
				&\leq \frac{1}{\sigma^2}\int_{|X-\mu|\geq k\sigma}(X-\mu)^2\ d\Prob.
			\end{split}
			\end{equation}
			Since $X$ has finite variance, the function $(X-\mu)^2$ is integrable. Consequently, the set function $E\mapsto \int_E(X-\mu)^2\ d\Prob$ is a measure absolutely continuous with respect to $\Prob$. This means that for any $\epsilon$, there is a $\delta$ so that $\Prob[E]<\delta$ implies that $\int_E(X-\mu)^2\ d\Prob<\epsilon$.\\

			\noindent Fix $\epsilon>0$. If we can show that for $k$ sufficiently large, $\Prob[|X-\mu|\geq k\sigma]<\delta$, then the last line in (\ref{mark}) will tend to zero as $k\to \infty$ by the above discussion. Consider the inequality
			\[
			\frac{|X-\mu|}{\sigma}-1 \leq \sum_{k=1}^\infty\ind_{|X-\mu|\geq k\sigma} \leq \frac{|X-\mu|}{\sigma}.
			\]
			Integrating through this inequality with respect to $\Prob$ and using the fact that $\Prob$ is a finite measure shows that $|X-\mu|/\sigma$ is integrable if and only if $\sum_{k=1}^\infty\Prob[|X-\mu|\geq k\sigma]<\infty$. Since $|X-\mu|/\sigma$ is indeed integrable, we have that $\Prob[|X-\mu|\geq k\sigma]\to 0$ as $k\to \infty$. We can then pick $k$ sufficiently large so that $\int_{|X-\mu|\geq k\sigma}(X-\mu)^2\ d\Prob<\epsilon$ so that
			\begin{align*}
			k^2\cdot \Prob[|X-\mu|\geq k\sigma] &\leq \frac{1}{\sigma^2}\int_{|X-\mu|\geq k\sigma}(X-\mu)^2\ d\Prob\\
			&\leq \frac{\epsilon}{\sigma^2}.
			\end{align*}
			We conclude that $\Prob[|X-\mu|\geq k\sigma]$ is $o(1/k^2)$ as $k\to \infty$.
		\end{proof}
	\end{enumerate}
\end{enumerate}

\end{document}