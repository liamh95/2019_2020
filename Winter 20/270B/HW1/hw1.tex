\documentclass[11pt,letterpaper]{report}
\usepackage{amssymb,amsfonts,color,graphicx,amsmath,enumerate,mathtools}
\usepackage{tikz}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{hyperref}

\newcommand{\naturals}{\mathbb{N}}
\newcommand{\integers}{\mathbb{Z}}
\newcommand{\complex}{\mathbb{C}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\rationals}{\mathbb{Q}}
\newcommand{\Lp}[2]{\left\|{#1}\right\|_{L^{#2}}}
\newcommand{\field}{\mathbb{F}}
\newcommand{\affine}{\mathbb{A}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\Var}{\text{Var}}
\newcommand{\ind}{\mathbbm{1}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\inprob}{\xrightarrow{\Prob}}
\newcommand{\weakly}{\xrightarrow{w}}
\renewcommand{\Re}{\text{Re}}

\newenvironment{solution}
{\begin{proof}[Solution]}
{\end{proof}}

\voffset=-3cm
\hoffset=-2.25cm
\textheight=24cm
\textwidth=17.25cm
\addtolength{\jot}{8pt}
\linespread{1.3}

\begin{document}
\noindent{\em Liam Hardiman\hfill{January 29, 2020 (Late)} }
\begin{center}
{\bf \Large 270B - Homework 1}
\vspace{0.2cm}
\hrule
\end{center}

\noindent\textbf{Problem 1. }Let $X_i$ be i.i.d. random variables each having the Poisson distribution with mean 1, and consider $S_n = X_1 + \cdots + X_n$. Let $x\in \reals$. Show that if $k = k(n)$ is such that $(k-n)/\sqrt{n}\to x$ as $n\to \infty$, we have
\[
\sqrt{2\pi n}\Prob[S_n = k] \to \exp(-x^2/2).
\]
\begin{proof}
	First we claim that $S_n$ has Poisson distribution with mean $n$. To see this, observe that by independence we have
	\begin{equation}\label{chfsum}
		\varphi_{S_n}(t) = \E\left[e^{it(X_1 + \cdots + X_n)}\right] = \E\left[e^{itX_1}\right]\cdots \E\left[e^{itX_n}\right] = \varphi_1(t)\cdots \varphi_n(t),
	\end{equation}
	where $\varphi_j$ is the characteristic function of $X_j$. Now if the random variable $X$ has Poisson distribution with intensity $\lambda$, its characteristic function is given by
	\begin{align*}
		\E\left[e^{itX}\right] = \sum_{k=0}^\infty e^{itk}\cdot \frac{\lambda^ke^{-\lambda}}{k!} = \exp(\lambda(e^{it}-1)).
	\end{align*}
	Using this, we see that
	\[
	\varphi_{S_n}(t) = \exp(e^{it}-1)^n = \exp(n(e^{it} - 1)),
	\]
	which is the characteristic function of the Poisson with intensity $\lambda$. Since a distribution is determined by its characteristic function, we conclude that $S_n$ has Poisson distribution with intensity $\lambda$.\\

	\noindent We use Stirling's approximation.
	\begin{align*}
		\sqrt{2\pi n}\Prob[S_n=k] = \sqrt{2\pi n}\frac{n^ke^{-n}}{k!}\sim \sqrt{\frac{n}{k}}\cdot \left(\frac{n}{k}\right)^k\cdot e^{k-n}.
	\end{align*}
	Taking the logarithm of both sides gives
	\begin{equation}\label{log}
	\log(\sqrt{2\pi n}\Prob[S_n=k]) \sim \frac{1}{2}\log\frac{n}{k} - k\log\frac{k}{n} + (k-n).
	\end{equation}
	We'll need to look at the asymptotic behavior of a few quantities to deal with this expression. Since $\frac{k-n}{\sqrt{n}}\to x$, we must have that $k\sim n$, so the first term in (\ref{log}) vanishes as $n\to \infty$. Since $k\sim n$, we can Taylor expand the logarithm in the middle term.
	\begin{equation}\label{remaminder}
	k\log\frac{k}{n} = k\log\left(1 + \frac{k-n}{n}\right) = k\left(\frac{k-n}{n} - \frac{1}{2}\left(\frac{k-n}{n}\right)^2 + O\left(\left(\frac{k-n}{n}\right)^3\right)\right).
	\end{equation}
	We have that $\frac{k-n}{\sqrt{n}} = x+o(1)$. From this it follows that
	\[
	k = n + \sqrt{n}(x+o(1)),\quad \frac{k-n}{n} = \frac{1}{\sqrt{n}}(x+o(1)). 
	\]
	Substituting these above gives
	\begin{align*}
		k\log \frac{k}{n} &= (n+\sqrt{n}(x+o(1)))\left(\frac{x+o(1)}{\sqrt{n}} - \frac{1}{2}\frac{(x+o(1))^2}{n} + O(n^{-3/2})\right)\\
		&= \sqrt{n}(x+o(1)) +\frac{1}{2}(x+o(1))^2 - \frac{1}{2}\frac{(x+o(1))^3}{\sqrt{n}} + O(n^{-1/2}).
	\end{align*}
	Finally, plugging this all back into (\ref{log}) gives
	\[
	\log(\sqrt{2\pi n}\Prob[S_n=k]) \sim-\frac{1}{2}(x+o(1))^2 + O(n^{-1/2}),
	\]
	which limits to $\exp(-x^2/2)$ as desired.
\end{proof}

\noindent\textbf{Problem 2. }
Find an example of random variables $X_n$ with densities $f_n$ so that $X_n$ converges weakly to the uniform distribution on $[0, 1]$ but $f_n(x)$ dos not converge to 1 for any $x\in [0,1]$.
\begin{solution}
	Let $f_{n,k}(x)$ be a typewriter sequence weighted to have integral 1, that is
	\[
	f_{n,k}(x) = \frac{1}{1-2^{-n}}\ind_{[0,1]\setminus [k\cdot 2^{-n},\ (k+1)\cdot 2^{-n}]}(x),\quad n = 1, 2, \ldots,\ k = 0, 1, \ldots, 2^{n}-1.
	\]
	Intuitively, $f_{n,k}$ is a flat line of height $\frac{1}{1-2^{-n}}$ over $[0,1]$ except for a gap at $[k\cdot 2^{-n}, (k+1)\cdot 2^{-n}]$, where it is zero. Since this gap slides along the unit interval indefinitely, $f_{n,k}$ does not converge pointwise anywhere. Now for any $\varphi$ bounded and continuous we have
	\[
	|\E_{n,k}[\varphi] - \E[\varphi]| = \int_0^1\varphi(x)(f_{n,k}(x) - 1)dx \leq \|\varphi\|_\infty\cdot 2^{-n}\to 0,
	\]
	so $X_n$ converges weakly to the uniform distribution on $[0,1]$.
\end{solution}

\noindent\textbf{Problem 3. }
Let $X_i$ be i.i.d. random variables each having exponential distribution with mean 1, and consider $M_n = \max_{i\leq n}X_i$. Show that $M_n - \log n$ converges weakly to the standard Gumbel distribution, i.e. the distribution with cumulative distribution function $F(x) = \exp(-e^{-x})$.
\begin{proof}
	For any $n$ and $t$ we have
	\[
	\Prob[M_n - \log n \leq t] = \Prob[M_n \leq \log n + t] = \Prob[X_i\leq \log n + t,\ 1\leq i\leq n].
	\]
	Since the $X_i$ are i.i.d. we can split this into a product
	\begin{align*}
	\Prob[M_n-\log n\leq t] &= \left(\Prob[X_i \leq \log_n + t]\right)^n\\
	&= \left(1 - \exp(-\log n  -t)\right)^n\\
	&= \left(1 - \frac{e^{-t}}{n}\right)^n\\
	&\to \exp(-e^{-t}),\quad \text{as }n\to \infty.
	\end{align*}
\end{proof}

\noindent\textbf{Problem 4. }Let $X_n$ be random variables and $c$ be a constant. Prove that weak convergence of $X_n$ to $c$ is equivalent to convergence of $X_n$ to $c$ in probability.
\begin{proof}
	Convergence in probability always implies weak convergence to the same limit, so we only need to show that weak convergence to $c$ implies convergence to $c$ in probability. The distribution function $F(t)$ of the random variable that is constantly $c$ is the shifted Heaviside function $F(t) = H(t-c)$. The only point of discontinuity of $F$ is at $t= a$. By the definition of weak convergence, if $F_n$ is the distribution function of $X_n$, we have that $F_n(x)\to F(x)$ for all $x\neq c$. For any $\epsilon>0$ we have
	\[
	\Prob[|X_n - c|<\epsilon] = F_n(c+\epsilon)-F_n(c-\epsilon)\to F(c+\epsilon) - F(c-\epsilon) = 1,
	\]
	so $X_n\to c$ in probability.
\end{proof}

\noindent\textbf{Problem 5. }
Consider the following statement:\\
\centerline{if $X_n\to X$ weakly and $Y_n\to Y$ weakly then $X_n+Y_n\to X+Y$ weakly.}
\begin{enumerate}[(a)]
	\item Find an example showing that this statement if false in general.
	\begin{solution}
		(I talked to Thomas Beardsley and Xiaowen Zhu about this one.) Let $X_n$ be a sequence of i.i.d. $\mcal{N}(0, 1)$ random variables. Set $Y_{2k} = X_{2k}$, but set $Y_{2k+1} = -X_{2k+1}$. Since the sequence of $X_n$'s are i.i.d., $X_n$ converges weakly to a standard normal random variable. Similarly, since the negative of a standard normal is a standard normal, $Y_n$ also converges weakly to a standard normal. However, the sequence $X_n + Y_n$ doesn't converge weakly at all since every odd term is identically zero and every even term has distribution $\mcal{N}(0, 4)$.
	\end{solution}

	\item Prove that if $Y$ is a constant, then the statement is true.
	\begin{proof}
		We'll show pointwise convergence of the characteristic functions.
		\begin{equation}\label{charf}
		\begin{split}
			\left|\E\left[e^{it(X_n+Y_n)} - e^{it(X+c)}\right]\right| &= \left|\E\left[e^{it(X_n+Y_n)} - e^{it(X_n+c)}+e^{it(X_n+c)}-e^{it(X+c)}\right]\right|\\
			& \leq \E\left|e^{itY_n} - e^{itc}\right| + e^{itc}\E\left[e^{itX_n} - e^{itX}\right].
		\end{split}
		\end{equation}
		Let's look at the first term. We split the region of integration like so
		\[
		\E\left|e^{itY_n} - e^{itc}\right| \cdot \ind_{|Y_n-c|\geq \epsilon} + \E\left|e^{itY_n} - e^{itc}\right| \cdot \ind_{|Y_n-c|< \epsilon}.
		\]
		Since weak convergence to a constant is equivalent to convergence in probability to a constant, the measure of the first region of integration goes to zero as $n\to \infty$, so the first term vanishes. Continuity of the exponential makes the second term small in $\epsilon$. Consequently, the first term on the last line of (\ref{charf}) can be made smaller than $\epsilon$ for $n$ large. The second term goes to zero since $X_n\to X$ weakly. Since the characteristic functions converge pointwise, we have weak convergence.
	\end{proof}

	\item Prove that if $X_n$ and $Y_n$ are independent then the statement is true.
	\begin{proof}
		We compute the characteristic functions and use independence to split the product.
		\begin{align*}
			\E\left[e^{it(X_n+Y_n)}\right] &= \E\left[ e^{itX_n}\right]\cdot \E\left[e^{itY_n}\right]\\
			&\to \E\left[e^{itX}\right]\E\left[e^{itY}\right].
		\end{align*}
		Since the characteristic functions converge pointwise, we have weak convergence.
	\end{proof}
\end{enumerate}

\noindent\textbf{Problem 6. }
\begin{enumerate}[(a)]
	\item Prove the following implication: if $X_n\to X$ weakly, $Y_n\geq 0$ and $Y_n\to c$ weakly where $c$ is a constant, then $X_nY_n\to cX$.
	\begin{proof}
		For any $0<\epsilon<c$ we split the probability
		\[
		\Prob[X_nY_n \leq t]= \Prob[X_nY_n \leq t,\ |Y_n-c] <\epsilon] + \Prob[X_nY_n \leq t,\ |Y_n-c|\geq \epsilon].
		\]
		Since weak convergence to a constant is equivalent to convergence to that constant in probability, the second term vanishes for $n$ large. Now we also have 
		\[
		\Prob[X_n Y_n \leq t,\ |Y_n-c|<\epsilon] \leq \Prob[X_n \leq \frac{t}{c-\epsilon},\ |Y_n-c|<\epsilon].
		\]
		Taking the $\liminf_{n\to \infty}$ on both sides, we have that $\liminf{n\to \infty}\Prob[X_nY_n \leq t] \leq \Prob[X \leq t/(c-\epsilon)]$ at all points of continuity of the distribution of $X$. Similarly,
		\[
		\Prob[X_nY_n \leq t,\ |Y_n - c|<\epsilon] \geq \Prob[X_n \geq \frac{t}{c+\epsilon},\ |Y_n - c| < \epsilon].
		\]
		Taking the $\limsup_{n\to \infty}$ on both sides, we have that $\limsup_{n\to \infty}\Prob[X_nY_n \leq t] \geq \Prob[X \leq t/(c+\epsilon)]$ at all points of continuity of the distribution of $X$. We conclude that $X_nY_n \to cX$ weakly.
		% I think the idea is to use $Y_n\geq 0$ to say
		% \[
		% \Prob[X_nY_n \leq t] = \Prob[X_n \leq t/Y_n].
		% \]
		% For any $\epsilon$ we can split this probability:
		% \[
		% \Prob[X_nY_n\leq t] = \Prob[X_n \leq t/Y_n,\ |Y_n-c|<\epsilon] + \Prob[X_n \leq t/Y_n,\ |Y_n-c|\geq \epsilon].
		% \]
		% Since $Y_n$ converges weakly to $c$, it converges to $c$ in probability as well, so the second term goes to zero as $n\to \infty$. Informally, I want to say that $t/Y_n$ will become close to $t/c$.
	\end{proof}

	\item Let $Z_n$ be a random vector uniformly distributed on the unit Euclidean sphere of radius $\sqrt{n}$ in $\reals^n$. Prove that the distribution of the first coordinate of $Z_n$ converges weakly to the standard normal distribution.
	\begin{proof}
		Let $X_n$ be a standard normal random vector and let $Z_n = X_n\cdot \sqrt{n}/\|X_n\|_2$. $X_n/\|X_n\|_2$ has norm 1, so $Z_n$ is definitely valued on the $\sqrt{n}$-sphere. Since $X_n$ is rotation invariant in distribution, so is $Z_n$, so we conclude that $Z_n$ is uniformly distributed on the $\sqrt{n}$-sphere.\\

		\noindent First we claim that $\sqrt{n}/\|X_n\| \weakly 1$. By Chebyshev we have that (a squared standard normal has mean 1, variance 2 and the components of $X$ are independent)
		\begin{align*}
			\Prob\left[\left|\frac{\|X_n\|^2}{n} - 1\right| > \epsilon\right] &= \Prob[|\|X_n\|^2 - n\| > n\epsilon]\\
			&\leq \frac{4n}{n^2\epsilon^2}\to 0.
		\end{align*}
		We conclude that $\|X_n\|^2/n$ converges to 1 in probability. If $f\inprob c\neq 0$, then $1/f \inprob 1/c$, so $n/\|X_n\|^2$ converges to 1 in probability, and therefore weakly. Since $x\mapsto \sqrt{x}$ is continuous, $\sqrt{n}/\|X_n\| \weakly 1$.\\

		\noindent Now the first component of $Z_n$ has distribution $g_n \sqrt{n}/\|X_n\|$, where $g_n$ is a standard normal random variable. $g_n$ converges to a standard normal weakly and we've shown that $\sqrt{n}/\|X_n\|\weakly 1$, so by part (a), the first component of $Z_n$ converges weakly to a standard normal.
	\end{proof}
\end{enumerate}

\noindent\textbf{Problem 8. }
Prov that if $\phi$ is a characteristic function of some random variable, then $\Re(\phi)$ and $|\phi|^2$ are too.
\begin{proof}
	Suppose $\phi_X$ is the characteristic function of the random variable $X$ and let $\zeta$ take values $\pm 1$ with probability $1/2$ independent of $X$. We then have
	\begin{align*}
		\phi_{X\zeta} = \E[e^{itX\zeta}] = \E[e^{itX}\cdot \ind_{\zeta= 1} + e^{-itX}\cdot\ind_{\zeta = -1}] = \frac{1}{2}(\phi_X + \overline{\phi_X}) = \Re(\phi_X).
	\end{align*}
	We conclude that $\Re(\phi_X)$ is the characteristic function of $X\zeta$.\\

	\noindent Let the random variable $Y$ have the same distribution as $-X$ independent of $X$. We then have
	\[
	\phi_{XY} = \E[e^{itXY}] = \E[e^{itX}]\E[e^{-itX}] = |\phi_X|^2.
	\]
	We conclude that $|\phi_X|^2$ is the characteristic function of $XY$.
\end{proof}

\noindent\textbf{Problem 9. }
Let $X$ be a random variable with characteristic function $\phi$. Prove that for any $a\in \reals$ we have
\[
\Prob[X = a] = \lim_{T\to \infty}\frac{1}{2T}\int_{T}^{-T}e^{-ita}\phi(t)\ dt.
\]

\noindent\textbf{Problem 11. }
Consider independent random variables $X_k$ such that $X_k$ takes values $\pm k$ with probability $k^{-2}/2$ each and values $\pm 1$ with probability $(1-k^{-2})/2$ each. Show that although $\Var[S_n]/n\to 2$, $S_n/\sqrt{n}$ does not converge to $\mcal{N}(0, 1)$ weakly. Why does this not contradict the Lindeberg-Feller central limit theorem?
\begin{proof}
	If $S_n/\sqrt{n}$ converged to $\mcal{N}(0, 1)$ weakly then its moments would converge to those of a standard normal. Let's look at the fourth moment of $S_n/\sqrt{n}$. Since each $X_k$ has mean zero and they're mutually independent, the only nonvanishing terms in the sum will have the form $X_i^2X_j^2$ for some $i, j$ (not necessarily distinct).
	\begin{align*}
		\E[(S_n/\sqrt{n})^4] &= \frac{1}{n^2}\E\left[\sum_{k=1}^nX_k^4 + \binom{n}{2}\sum_{1\leq i<j\leq n}X_i^2X_j^2\right]\\
		&\geq \frac{1}{n^2}\sum_{k=1}^n\E[X_k^4]\\
		&= \frac{1}{n^2}\sum_{k=1}^n\left(k^2 + (1-1/k^2)\right)\\
		&= \Theta(n) \to \infty.
	\end{align*}
	But $\mcal{N}(0, 1)$ has finite moments of all orders. We conclude that $S_n/\sqrt{n}$ does not converge weakly to $\mcal{N}(0, 1)$.\\

	\noindent The problem is that this sequence of random variables does not satisfy the Lindeberg condition:
	\[
	\frac{1}{n}\sum_{k=1}^n\E[X_k^2\cdot \ind_{|X_k| > \sqrt{n}\epsilon}] \to 0
	\]
	for all $\epsilon$. As $n$ grows, the last term contributes $\Theta(n)$ to this sum, so when divided by $n$ the result is $\omega(1)$, which doesn't go to zero. 
\end{proof}

\end{document}