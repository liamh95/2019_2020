%% Please change the file name by replacing N with the apporpriate number
%% corresponding to the current homework and XX with your initials.
%% https://www.math.uci.edu/~gpatrick/jsOnline/hw1.html

\documentclass[11pt,letterpaper]{report}
\usepackage{amssymb,amsfonts,color,graphicx,amsmath,enumerate}
\usepackage{tikz} %This package offers the ability to draw pictures
\usepackage{amsthm}
\usepackage{bbm}

\newcommand{\naturals}{\mathbb{N}}
\newcommand{\integers}{\mathbb{Z}}
\newcommand{\complex}{\mathbb{C}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\exreals}{\overline{\mathbb{R}}}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\mable}{measurable}
\newcommand{\quats}{\mathbb{H}}
\newcommand{\rationals}{\mathbb{Q}}
\newcommand{\norm}{\trianglelefteq}
\newcommand{\Aut}{\text{Aut}}
\newcommand{\disk}{\mathbb{D}}
\newcommand{\halfplane}{\mathbb{H}}
\newcommand{\Lp}[2]{\left\|{#1}\right\|_{L^{#2}}}
\newcommand{\supp}[1]{\text{supp}({#1})}
\newcommand{\Hom}[2]{\text{Hom}_{{#1}}({#2})}
\newcommand{\tr}{\text{tr}}
\newcommand{\field}[1]{\mathbb{F}_{{#1}}}{}
\newcommand{\Gal}[1]{\text{Gal}\left({#1}\right)}
\newcommand{\esssup}{\text{ess sup }}
\newcommand{\essinf}{\text{ess inf }}
\newcommand{\affine}{\mathbb{A}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\prob}{\mathbb{P}}{}
\newcommand{\Var}{\text{Var}}
\newcommand{\cov}{\text{cov}}
\newcommand{\ind}{\mathbbm{1}}

\newenvironment{solution}
{\begin{proof}[Solution]}
{\end{proof}}

\voffset=-3cm
\hoffset=-2.25cm
\textheight=24cm
\textwidth=17.25cm
\addtolength{\jot}{8pt}
\linespread{1.3}

\begin{document}
\noindent{\em Liam Hardiman\hfill{October 23, 2019} }
% Please give relevant information
\begin{center}
{\bf \Large 271A - Homework 2} %Replace N with the appropriate number
\vspace{0.2cm}
\hrule
\end{center}

\begin{enumerate}
	\item Use Jensen's inequality to show that for $p\geq 1$.
	\[
	\Lp{\E[X|\mcal{G}]}{p} \leq \Lp{X}{p}.
	\]
	\begin{proof}
		Let's look at the $p$-norm to the $p$-th power.
		\begin{align*}
			\Lp{\E[X|\mcal{G}]}{p}^p &= \int |\E[X|\mcal{G}]|^p\ d\prob\\
			&\leq \int\E[|X|^p|\mcal{G}]\ d\prob\quad\text{(by Jensen's inequality)}\\
			&= \int |X|^p\ d\prob\quad\text{(by definition of conditional expectation)}\\
			&= \Lp{X}{p}^p.
		\end{align*}
		Taking the $p$-th root of both sides establishes the claim.
	\end{proof}

	\item Let $(X_n: n\in \naturals)$ be a sequence of independent random variables, each exponentially distributed:
	\[
	\prob[X_n>x] =e^{-x},\quad x\geq 0.
	\]
	\begin{enumerate}
		\item A random variable $\tau$ has the lack of memory property if
		\[
		\prob[\tau>a+b\ |\ \tau>a] = \prob[\tau>b].
		\]
		Show that a random variable has the memoryless property if and only if it is exponentially distributed.
		\begin{proof}
			Suppose $\tau$ is exponentially distributed, i.e.
			\[
			\prob[\tau > x] = \begin{cases}
				e^{-\lambda x},&\text{if }x\geq 0,\\
				1,&\text{if }x <0.
			\end{cases}
			\]
			By the definition of conditional probability we have 
			\begin{align*}
				\prob[\tau>a+b\ |\ \tau > a] &= \frac{\prob[(\tau>a+b) \land (\tau>a)]}{\prob[\tau>a]}\\
			\end{align*}
			Now if $b\geq 0$, $\prob[(\tau>a+b)\land (\tau>a)] = \prob[\tau>a+b]$. This gives
			\begin{align*}
				\prob[\tau>a+b\ |\ \tau > a] &= \frac{\prob[\tau>a+b]}{\prob[\tau>a]}\\
				&= \frac{e^{-\lambda(a+b)}}{e^{-\lambda a}}\\
				&= e^{-\lambda b}\\
				&= \prob[\tau>b].
			\end{align*}
			On the other hand, if $b<0$, $\prob[(\tau>a+b)\land(t>a)] = \prob[\tau>a]$, which gives
			\begin{align*}
				\prob[\tau>a+b\ |\ \tau > a] &=\frac{\prob[\tau>a]}{\prob[\tau>a]}\\
				&= 1\\
				&= \prob[\tau>b].
			\end{align*}
			Conversely, suppose that $\tau$ is memoryless. If $b$ is positive then we have
			\[
			\prob[\tau>a+b] = \prob[\tau>a]\cdot \prob[\tau>b].
			\]
			If we let $F(x)= \prob[\tau>x]$, then $F$ satisfies the exponential property:
			\[
			F(a+b) = F(a)F(b).
			\]
			Setting $a=b$, we have $F(2a) = F(a)^2$. Inductively, we obtain $F(na) = F(a)^n$ for any positive integer $n$. Taking the $n$-th root of both sides gives $F(a/n) = F(a)^{1/n}$. Combining these gives $F(\frac{m}{n}a) = F(a)^{m/n}$. For any rational $\frac{m}{n}$.\\

			\noindent Since any real number is a limit of rational numbers, we obtain $F(ra)= F(a)^{r}$ for any real $r$ by continuity. Since this holds for any $a\geq 0$, we can set $a=1$ to obtain $F(r) = F(1)^r$, so $F$, the distribution of $\tau$, is exponential.
		\end{proof}

		\item Compute the expectation and variance of $X_n$. Let $Y = X_n + X_{n+1}$. Find the correlation coefficient between $Y$ and $X_n$. Find $\E[Y|X_{n+1}]$.
		\begin{solution}
			Let's compute the first two moments.
			\begin{align*}
				\E[X_n] &= \int_0^\infty e^{-x}\ dx\\
				&= 1.
			\end{align*}
			\begin{align*}
				\E[X_n^2] &= 2\int_0^\infty xe^{-x}\ dx\\
				&= 2.
			\end{align*}
			We then have $\E[X_n] = 1$ and $\Var[X_n] = \E[X_n^2] -\E[X_n]^2 = 1$.\\

			\noindent Let $Y = X_n+X_{n+1}$. We'll need the mean and variance of $Y$ to compute the correlation coefficient.
			\begin{align*}
				\E[Y] &= \E[X_n] + \E[X_{n+1}] = 2.
			\end{align*}
			Since $X_n$ and $X_{n+1}$ are independent, we also have
			\[
			\Var[Y] = \Var[X_n] + \Var[X_{n+1}] = 2.
			\]
			Now let's compute the correlation coefficient.
			\begin{align*}
				\rho_{Y, X_{n+1}} &= \frac{\E[(Y - \mu_Y)(X_{n+1} - \mu_{n+1})]}{\sigma_Y\sigma_{n+1}}\\
				&= \frac{\E[YX_{n+1}] - \mu_{n+1}\E[Y] - \mu_Y\E[X_{n+1}]  +\mu_Y\mu_{n+1}}{\sqrt{2}}\\
				&= \frac{\E[X_nX_{n+1}] + \E[X_{n+1}^2]-2-2+2}{\sqrt{2}}\\
				&= \frac{1}{\sqrt{2}}.
			\end{align*}
			Now for the conditional expectation. Since conditional expectation is linear and $X_n$ and $X_{n+1}$ are independent,
			\begin{align*}
				\E[Y|X_{n+1}] &= \E[X_n|X_{n+1}] + \E[X_{n+1}|X_{n+1}]\\
				&= \E[X_n] + X_{n+1}\\
				&= 1+X_{n+1}.
			\end{align*}
		\end{solution}
		\item Show that
		\[
		\prob[X_n > \alpha \log n\text{ for infinitely many }n] = \begin{cases}
			0&\text{for }\alpha>1,\\
			1&\text{else}
		\end{cases}.
		\]
		\begin{proof}
			Let $E_n$ be the event $E_n = \{X_n > \alpha \log n\}$. Let's sum these events
			\begin{align*}
				\sum_{n=1}^\infty \prob[E_n] &= \sum_{n=1}^\infty e^{-\alpha \log n}\\
				&= \sum_{n=1}^\infty \frac{1}{n^\alpha}.
			\end{align*}
			This is summable if and only if $\alpha>1$. By Borel-Cantelli, we have
			\[
			\prob[\limsup E_n] = \prob[X_n>\alpha \log n\text{ for infinitely many }n] = 0.
			\]
			Since the $X_n$'s are given to be independent, if the above sum diverges, $\prob[\limsup E_n] = 1$ by the converse of Borel-Cantelli.
		\end{proof}
	\end{enumerate}

	\item Let $B_t$ be a standard Brownian motion.
	\begin{enumerate}
		\item Find a matrix $A$ so that in distribution $y = [B_{t_1}, B_{t_2}, \ldots, B_{t_n}]^T = A[Z_1, Z_2, \ldots, Z_n]^T$, where the $Z_j$'s are iid standard normal random variables.
		\begin{solution}
			Call $x = [Z_1, \ldots, Z_n]^T$. The $i$-th component of the product $Ax$ has distribution $(Ax)_i\sim \mcal{N}(0, a_{i,1}^2 + \cdots + a_{i,n}^2)$. Since $y_i = B_{t_i}\sim \mcal{N}(0, t_i)$ and we want $y = Ax$, we must then have
			\begin{equation}\label{rows}
				a_{i,1}^2 + \cdots + a_{i,n}^2 = t_i
			\end{equation}
			Without loss of generality and for notational convenience, assume that $t_1\leq t_2\leq \cdots \leq t_n$. We know that $\cov(B_{t_i}, B_{t_j}) = t_i\land t_j = t_{i\land j}$, so, we know that the matrix $\cov(y)$ has entries $\cov(y)_{i,j} = t_{i\land j}$. We also know that
			\[
			\cov(Ax) = A\cdot \cov(x)\cdot A^T = AA^T.
			\]
			Since we want $y = Ax$, we must then have $(AA^T)_{i,j} = t_{i\land j}$. Note that this implies (\ref{rows}).\\

			\noindent In summary, we've reduced the problem to finding $A$ subject to $(AA^T)_{i,j} = t_{i\land j}$. The matrix $t_{i\land j}$ is clearly symmetric and it's positive definite by our reordering of the $t_i$'s and Sylvester's criterion. If we define the matrix $T$ to have entries $(T)_{i,j} = t_{i\land j}$, we can diagonalize $T$:
			\[
				T = UDU^T,
			\]
			where $U$ is orthogonal and $D$ is diagonal with nonnegative entries. If we set $A = UD^{1/2}$, then $AA^T = T$ as desired.
		\end{solution}

		\item Show that $e^{-at}B_{e^{2at}}$ is a Gaussian process. Find its covariance function.
		\begin{solution}
			This immediately follows from part (a). For any $t_1, \ldots, t_n$ there is a matrix $A$ so that
			\[
			[B_{e^{2at_1}}, \ldots, B_{e^{2at_n}}]^T = A[Z_1, \ldots, Z_n]^T,
			\]
			where the $Z_i$'s are iid standard normal random variables. Multiply both sides by the diagonal matrix whose $i$-th entry is $e^{-at_i}$ and we're done.\\

			\noindent Let's compute the covariance function.
			\begin{align*}
				\text{cov}(s,t) &= \cov(e^{-as}B_{e^{2as}}, e^{-at}B_{e^{2at}})\\
				&= e^{-a(s+t)}\E[B_{e^{2as}}B_{e^{2at}}]\\
				&= e^{-a(s+t)}(e^{2as}\land e^{2at}).
			\end{align*}
		\end{solution}
	\end{enumerate}
	\item Consider the random vector $x = [X_1, \ldots, X_n]\in \reals^n$.
	\begin{enumerate}
		\item Prove Chebyshev's inequality: for $p>0$ we have
		\[
		\prob\left[\sum_i|X_i|^p\geq \lambda^p\right]\leq \lambda^{-p}\E\left[\sum_i|X_i|^p\right].
		\]
		\begin{proof}
			Let's integrate.
			\begin{align*}
				\lambda^{-p}\E\left[\sum_i|X_i|^p\right]&\geq \lambda^{-p}\E\left[\sum_i|X_i|^p\cdot \ind_{\sum_i|X_i|^p \geq \lambda^p} \right]\\
				&\geq \lambda^{-p}\cdot \lambda^p\cdot \prob\left[\sum_i|X_i|^p\geq \lambda^p\right].
			\end{align*}
		\end{proof}

		\item Suppose there exists $k>0$ so that
		\[
		M = \E[\exp(k(\sum_i|X_i|^p)^{1/p})]<\infty.
		\]
		Prove that $\prob[\sum_i|X_i|^p\geq \lambda^p]\leq Me^{-k\lambda}$ for all $\lambda\geq 0$.
		\begin{proof}
			The function $t\mapsto e^{kt^{1/p}}$ is nondecreasing, so if we let $E_\lambda = \{\sum_i|X_i|^p\geq \lambda^p\}$, then
			\begin{align*}
				0\leq e^{k\lambda}\ind_{E_\lambda}\leq \exp(k(\sum_i|X_i|^p)^{1/p})]\ind_{E_\lambda}.
			\end{align*}
			Now we just take the expectation.
			\begin{align*}
				e^{k\lambda}\prob[E_\lambda]\leq M.
			\end{align*}
		\end{proof}
	\end{enumerate}

	\item Let $\Omega = \{1, 2, 3, 4, 5\}$ and let $\mcal{U}$ be the collection
	\[
	\mcal{U} = \{\{1, 2, 3\}, \{3, 4, 5\}\},
	\]
	of subsets of $\Omega$.
	\begin{enumerate}
		\item Find $\sigma(\mcal{U})$, the $\sigma$-algebra generated by $\mcal{U}$.
		\begin{solution}
			Taking complements gives us $\{4, 5\}$ and $\{1, 2\}$. Intersecting gives $\{3\}$. We can take a union to get $\{1, 2, 4, 5\}$. We can't get anything new from unions, intersections, or complements here, so we conclude that
			\[
			\sigma(\mcal{U}) = \{\{1, 2, 3\}, \{3, 4, 5\}, \{3\}, \{4, 5\}, \{1, 2\}, \{1, 2, 4, 5\}, \Omega, \emptyset\}.
			\]
		\end{solution}

		\item Define a random variable by $X(1) = X(2) = 0$, $X(3) = 10$, $X(4) = X(5) = 1$. Is $X$ measurable wrt $\sigma(\mcal{U})$?
		\begin{solution}
			$X$ is $\sigma(\mcal{U})$ measurable.
			\[
			X^{-1}(-\infty, \alpha] = \begin{cases}
				\emptyset,&\alpha<0\\
				\{1, 2\},&\alpha\in[0,1)\\
				\{1,2,4,5\},&\alpha\in[1, 10)\\
				\Omega,&\alpha\geq 10
			\end{cases}.
			\]
			Each of these preimages is measurable, so $X$ is measurable.
		\end{solution}

		\item Define another random variable by $Y(1) = 0$, $Y(2) = Y(3) = Y(4) = Y(5) = 1$. Find the $\sigma$-algebra generated by $Y$.
		\begin{solution}
			$\sigma(Y)$ is the $\sigma$-algebra generated by sets of the form $Y^{-1}(-\infty, \alpha]$ for $\alpha\in \reals$. As $\alpha$ ranges of $\reals$, we pick up the sets $\emptyset$, $\{1\}$, and $\Omega$. Taking complements gives
			\[
			\sigma(Y) = \{\emptyset, \{1\}, \{2, 3,4, 5\}, \Omega\}.
			\]
		\end{solution}
	\end{enumerate}
\end{enumerate}

\end{document}