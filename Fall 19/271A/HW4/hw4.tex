\documentclass[11pt,letterpaper]{report}
\usepackage{amssymb,amsfonts,color,graphicx,amsmath,enumerate}
\usepackage{tikz}
\usepackage{amsthm}
\usepackage{bbm}

\newcommand{\naturals}{\mathbb{N}}
\newcommand{\integers}{\mathbb{Z}}
\newcommand{\complex}{\mathbb{C}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\exreals}{\overline{\mathbb{R}}}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\mable}{measurable}
\newcommand{\quats}{\mathbb{H}}
\newcommand{\rationals}{\mathbb{Q}}
\newcommand{\norm}{\trianglelefteq}
\newcommand{\Aut}{\text{Aut}}
\newcommand{\disk}{\mathbb{D}}
\newcommand{\halfplane}{\mathbb{H}}
\newcommand{\Lp}[2]{\left\|{#1}\right\|_{L^{#2}}}
\newcommand{\supp}[1]{\text{supp}({#1})}
\newcommand{\Hom}[2]{\text{Hom}_{{#1}}({#2})}
\newcommand{\tr}{\text{tr}}
\newcommand{\field}[1]{\mathbb{F}_{{#1}}}
\newcommand{\Gal}[1]{\text{Gal}\left({#1}\right)}
\newcommand{\esssup}{\text{ess sup }}
\newcommand{\essinf}{\text{ess inf }}
\newcommand{\affine}{\mathbb{A}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\Var}{\text{Var}}
\newcommand{\ind}{\mathbbm{1}}
\newcommand{\Cov}{\text{Cov}}

\newenvironment{solution}
{\begin{proof}[Solution]}
{\end{proof}}

\voffset=-3cm
\hoffset=-2.25cm
\textheight=24cm
\textwidth=17.25cm
\addtolength{\jot}{8pt}
\linespread{1.3}

\begin{document}
\noindent{\em Liam Hardiman\hfill{November 6, 2019} }
\begin{center}
{\bf \Large 271A - Homework 4}
\vspace{0.2cm}
\hrule
\end{center}

\section*{Problem 1. }
Let $B_t$ be a Brownian motion. Show that
\[
Y_t = (1+t)B_{(1+t)^{-1}}-B_1
\]
is a Brownian motion on $[0, \infty)$.
\begin{proof}
	We have $Y_0 = B_1 - B_1 = 0$. Since $t\mapsto \frac{1}{1+t}$ is continuous and $t\mapsto B_t$ is a.s. continuous, we have that $t\mapsto (1+t)B_{(1+t)^{-1}}-B_1$ is a.s. continuous. We also have by the linearity of expectation that $\E[Y_t] = 0$ for all $t$. Let $s<t$ and consider the increment $Y_t - Y_s$. Since $\frac{1}{1+t}<\frac{1}{1+s}$ we have
	\begin{align*}
		Y_t - Y_s &= (1+t)B_{(1+t)^{-1}} - (1+s)B_{(1+s)^{-1}}\\
		&=(1+t)B_{(1+t)^{-1}} - (1+s)B_{(1+s)^{-1}} + (1+s)B_{(1+t)^{-1}}-(1+s)B_{(1+t)^{-1}}\\
		&= (t-s)B_{(1+t)^{-1}} - (1+s)[B_{(1+s)^{-1}}-B_{(1+t)^{-1}}].
	\end{align*}
	Since $B$ is a Brownian motion, the increments $B_{(1+t)^{-1}}$ and $B_{(1+s)^{-1}}-B_{(1+t)^{-1}}$ are independent. Since the above is a sum of independent Gaussians, the increment $Y_t-Y_s$ is also Gaussian. The linearity of expectation gives $\E[Y_t-Y_s] = 0$ and the variance is given by
	\begin{align*}
		\Var[Y_t-Y_s] &= (t-s)^2\cdot \Var[B_{(1+t)^{-1}}] + (1+s)^2\cdot \Var[B_{(1+s)^{-1}}-B_{(1+t)^{-1}}]\\
		&= \frac{(t-s)^2}{1+t} + (1+s)^2\left(\frac{1}{1+s}-\frac{1}{1+t}\right)\\
		&= t-s.
	\end{align*}
	It remains to show that the increments of $Y$ are independent. To this end, let $0\leq s_1<t_1\leq \cdots \leq s_n<t_n$. Since the increments $Y_{t_i}-Y_{s_i}$ are Gaussian, it suffices to show that they are uncorrelated. Suppose $i<j$ we then have
	\begin{align*}
		\Cov(Y_{t_i}-Y_{s_i}, Y_{t_j}-Y_{s_j})&\\
		&= (1+t_i)(1+t_j)\left(\frac{1}{1+t_i}\land \frac{1}{1+t_j}\right) - (1+t_i)(1+s_j)\left(\frac{1}{1+t_i}\land \frac{1}{1+s_j}\right)\\
		&-(1+s_i)(1+t_j)\left(\frac{1}{1+s_i}\land \frac{1}{1+t_j}\right)+(1+s_i)(1+s_j)\left(\frac{1}{1+s_i}\land \frac{1}{1+s_j}\right)\\
		&= 0.
	\end{align*}
	THus, the increments are independent and we conclude that $Y$ is a Brownian motion.
\end{proof}


\noindent\textbf{Problem 2. }
Consider the random walk
\[
S_n = \sum_{i=1}^n\zeta_i
\]
for $\Prob[\zeta_i=1] = p$ and $\Prob[\zeta_i = -1] = 1-p$ and the $\zeta_i$ are independent. Given $\lambda$, find $\gamma$ so that
\[
\exp(\gamma S_n - \lambda n)
\]
is a martingale with respect to the filtration generated by the $\zeta_i$'s.
\begin{solution}
	Let $M_n = \exp(\gamma S_n - \lambda n)$ and let $\mcal{F}_n$ be the filtration generated by $\zeta_1, \ldots, \zeta_n$. We compute the conditional expectation
	\begin{align*}
		\E[M_{n+1}\ |\ \mcal{F}_n] &= \E[\exp(\gamma S_n - \lambda n)\cdot \exp(\gamma \zeta_{n+1}-\lambda)\ |\ \mcal{F}_n]\\
		&= M_n\cdot \E[\exp(\gamma\zeta_{n+1}-\lambda)].
	\end{align*}
	Here we've used that $M_n$ is $\mcal{F}_n$-measurable and $\zeta_{n+1}$ is independent of $\mcal{F}_n$. In order for $M$ to be a martingale, we need $\E[\exp(\gamma \zeta_{n+1}-\lambda)] = 1$. Let's make it happen.
	\begin{align*}
		&\E[\exp(\gamma\zeta_{n+1}-\lambda)] = 1\\
		\iff & pe^{\gamma-\lambda} + (1-p)e^{-\gamma-\lambda} = 1\\
		\iff & pe^{2\gamma} - e^{\lambda}e^{\gamma} + (1-p) = 0\\
		\iff & e^{\gamma} = \frac{e^\lambda \pm \sqrt{e^{2\lambda}-4p(1-p)}}{2p}\\
		\iff & \gamma = \log \frac{e^\lambda \pm \sqrt{e^{2\lambda}-4p(1-p)}}{2p}.
	\end{align*}
	Of course, this expression makes sense only when the numerator is positive and $e^{2\lambda}\geq 4p(1-p)$.
\end{solution}


\noindent\textbf{Problem 3. }
Assume that $X_n\to X$ in probability and $X_n\to Y$ a.s. Show that $X=Y$ a.s.
\begin{proof}
	First, we claim that since $X_n\to X$ in probability, every subsequence of $X_n$ contains a further subsequence converging to $X$ a.s. Since the a.s. limit is unique, we can conclude that $X = Y$ a.s.\\

	\noindent To prove our claim, we fix a subsequence $X_{n_k}$. Since $X_n\to X$ in probability, there is some $n_{k_1}$ so that for all $n$ at least $n_{k_1}$,
	\[
	\Prob[|X_n-X|>2^{-1}] \leq 2^{-1}.
	\]
	Inductively, suppose that we've constructed $n_{k_1} < \cdots < n_{k_m}$ so that
	\[
	\Prob[|X_{n_{k_i}}-X|>2^{-i}] \leq 2^{-i}
	\]
	for $i = 1, \ldots, m$. Again by convergence in probability, we can choose $n_{k_{m+1}}>n_{k_m}$ so that
	\[
	\Prob[|X_{n_{k_{m+1}}}-X|>2^{-(m+1)}]\leq 2^{-(m+1)}.
	\]
	If we let $A_m$ be the event $\{|X_{n_{k_m}}-X| > 2^{-m}\}$, then $\Prob[A_m]$ is summable by construction. By Borel-Cantelli, $\Prob[\limsup A_m] = 0$. Consequently, we have
	\[
	\Prob\left[\bigcup_{\ell\in \naturals}\bigcap_{m\geq \ell}\{|X_{n_{k_m}}-X|>2^{-m}\}\right] = 1,
	\]
	so $X_{n_{k_m}}\to X$ a.s.
\end{proof}

\noindent\textbf{Problem 4. }
\begin{enumerate}[(a.)]
	\item Show that a continuous stochastic process with non-zero and finite 2-variation has infinite 1-variation.
	\begin{proof}
		Let $X_t$ be a continuous stochastic process with nonzero finite quadratic variation. We then have for any partition $\Pi = \{0 = t_0 < t_1<\cdots < t_n = t\}$,
		\begin{align*}
			0 & < \sum_{k=1}^n|X_{t_k}-X_{t_{k-1}}|^2\\
			&\leq \max_{k \leq n}|X_{t_k}-X_{t_{k-1}}|\cdot \sum_{k=1}^n|X_{t_k}-X_{t_{k-1}}|\\
			&\leq \max_{|s-t|<\|\Pi\|}|X_s-X_t|\cdot \sum_{k=1}^n|X_{t_k}-X_{t_{k-1}}|.
		\end{align*}
		By continuity, we have $\lim_{\|\Pi\|\to 0}\max_{|s-t|<\|\Pi\|}|X_s-X_t| = 0$, so dividing the above inequality through by $\max_{|s-t|<\|\Pi\|}|X_s-X_t|$ and sending $\|\Pi\|\to 0$ establishes that the total variation of $X$ must be infinite.
	\end{proof}

	\item Show that a continuous process with finite total variation has zero quadratic variation.
	\begin{proof}
		With $X$ defined as in part (a.), the same reasoning gives
		\[
		\sum_{k=1}^n|X_{t_k}-X_{t_{k-1}}|^2\leq \max_{|s-t|<\|\Pi\|}|X_s-X_t|\cdot \sum_{k=1}^n|X_{t_k}-X_{t_{k-1}}|.
		\]
		As we send $\|\Pi\|\to 0$, the left-hand side approaches the quadratic variation. The $\max$ term on the right goes to zero by continuity and the sum on the right approaches the total variation, which is assumed to be finite. Consequently, the right-hand side limits to zero, so the quadratic variation of $X$ is zero.
	\end{proof}

	\item Show that the total variation of a Brownian motion is infinite a.s.
	\begin{proof}
		We showed in class that for a Brownian motion $B$, $L_t^{(2)}(B) = t$ a.s. By part (a.), we must have that $L_t^{(1)}(B)$ is infinite a.s.
	\end{proof}
\end{enumerate}

\noindent\textbf{Problem 5. }
Consider the compound Poisson process
\[
X_t = \sum_{i=1}^{N_t}Y_i,
\]
with $N_t$ Poisson with parameter $\lambda$ and independent of the $Y_i$ which are centered iid with variance $\sigma^2$.
\begin{enumerate}[(a.)]
	\item Show that $X_s$ and $X_t-X_s$ are independent.
	\begin{proof}
		Suppose $s<t$. We then have that
		\[
		X_s = \sum_{i=1}^{N_s}Y_i,\qquad X_t-X_s = \sum_{i=N_s+1}^{N_t}Y_i.
		\]
		These sums are over disjoint indices and $N$ has independent increments. Since the $Y_i$'s are independent, we conclude that $X_s$ and $X_t-X_s$ are sums of mutually independent random variables, and are hence independent.
	\end{proof}

	\item Find the quadratic variation, $\langle X\rangle_t$, associated with the process.
	\begin{solution}
		Fix $\Pi = \{0 = t_0 < t_1 <\cdots < t_n = t\}$. We then have
		\begin{equation}\label{qvar}
			V_t^{(2)}(\Pi, X) = \E\left[\sum_{i=1}^n|X_{t_i}-X_{t_{i-1}}|^2\right] = \sum_{i=1}^n\E[X_{t_i}^2-2X_{t_i}X_{t_{i-1}}+X_{t_{i-1}}^2].
		\end{equation}
		We showed on the last homework assignment that $\E[X_t^2] = \lambda t\sigma^2$. Let's compute the cross term. Since the $Y_i$'s are independent, we have
		\begin{align*}
			\E[X_{t_i}X_{t_{i-1}}] &= \E\left[\left(\sum_{j=1}^{N_{t_i}}Y_j\right)\left(\sum_{k=1}^{N_{t_{i-1}}}Y_k\right)\right]\\
			&= \E\left[\sum_{j=1}^{N_{t_{i-1}}}Y_j^2\right]\\
			&= \lambda t_{i-1}\sigma^2.
		\end{align*}
		Returning to (\ref{qvar}), we then have
		\begin{align*}
		V_t^{(2)}(\Pi, X) &= \lambda\sigma^2 \sum_{i=1}^n|t_i-t_{i-1}|\\
		&= \lambda\sigma^2t.
		\end{align*}
		Since this quantity is independent of the partition $\Pi$, we conclude that $\langle X\rangle_t = \lambda\sigma^2t$.
	\end{solution}

	\item Compute $\E[X_t^2 - \langle X\rangle_t]$.
	\begin{solution}
		By the linearity of expectation we have
		\begin{align*}
			\E[X_t^2 - \langle X\rangle_t] &= \E[X_t^2] - \E[\langle X\rangle_t]\\
			&= \lambda\sigma^2t - \lambda\sigma^2t\\
			&= 0.
		\end{align*}
	\end{solution}
\end{enumerate}

\end{document}