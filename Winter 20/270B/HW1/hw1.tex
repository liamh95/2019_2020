\documentclass[11pt,letterpaper]{report}
\usepackage{amssymb,amsfonts,color,graphicx,amsmath,enumerate}
\usepackage{tikz}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{hyperref}

\newcommand{\naturals}{\mathbb{N}}
\newcommand{\integers}{\mathbb{Z}}
\newcommand{\complex}{\mathbb{C}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\rationals}{\mathbb{Q}}
\newcommand{\Lp}[2]{\left\|{#1}\right\|_{L^{#2}}}
\newcommand{\field}{\mathbb{F}}
\newcommand{\affine}{\mathbb{A}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\Var}{\text{Var}}
\newcommand{\ind}{\mathbbm{1}}
\newcommand{\Cov}{\text{Cov}}

\newenvironment{solution}
{\begin{proof}[Solution]}
{\end{proof}}

\voffset=-3cm
\hoffset=-2.25cm
\textheight=24cm
\textwidth=17.25cm
\addtolength{\jot}{8pt}
\linespread{1.3}

\begin{document}
\noindent{\em Liam Hardiman\hfill{January 29, 2020 (Late)} }
\begin{center}
{\bf \Large 270B - Homework 1}
\vspace{0.2cm}
\hrule
\end{center}

\noindent\textbf{Problem 1. }Let $X_i$ be i.i.d. random variables each having the Poisson distribution with mean 1, and consider $S_n = X_1 + \cdots + X_n$. Let $x\in \reals$. Show that if $k = k(n)$ is such that $(k-n)/\sqrt{n}\to x$ as $n\to \infty$, we have
\[
\sqrt{2\pi n}\Prob[S_n = k] \to \exp(-x^2/2).
\]
\begin{proof}
	First we claim that $S_n$ has Poisson distribution with mean $n$. To see this, observe that by independence we have
	\begin{equation}\label{chfsum}
		\varphi_{S_n}(t) = \E\left[e^{it(X_1 + \cdots + X_n)}\right] = \E\left[e^{itX_1}\right]\cdots \E\left[e^{itX_n}\right] = \varphi_1(t)\cdots \varphi_n(t),
	\end{equation}
	where $\varphi_j$ is the characteristic function of $X_j$. Now if the random variable $X$ has Poisson distribution with intensity $\lambda$, its characteristic function is given by
	\begin{align*}
		\E\left[e^{itX}\right] = \sum_{k=0}^\infty e^{itk}\cdot \frac{\lambda^ke^{-\lambda}}{k!} = \exp(\lambda(e^{it}-1)).
	\end{align*}
	Using this, we see that
	\[
	\varphi_{S_n}(t) = \exp(e^{it}-1)^n = \exp(n(e^{it} - 1)),
	\]
	which is the characteristic function of the Poisson with intensity $\lambda$. Since a distribution is determined by its characteristic function, we conclude that $S_n$ has Poisson distribution with intensity $\lambda$.\\

	\noindent We use Stirling's approximation.
	\begin{align*}
		\sqrt{2\pi n}\Prob[S_n=k] = \sqrt{2\pi n}\frac{n^ke^{-n}}{k!}\sim \sqrt{\frac{n}{k}}\cdot \left(\frac{n}{k}\right)^k\cdot e^{k-n}.
	\end{align*}
	Taking the logarithm of both sides gives
	\begin{equation}\label{log}
	\log(\sqrt{2\pi n}\Prob[S_n=k]) \sim \frac{1}{2}\log\frac{n}{k} - k\log\frac{k}{n} + (k-n).
	\end{equation}
	We'll need to look at the asymptotic behavior of a few quantities to deal with this expression. Since $\frac{k-n}{\sqrt{n}}\to x$, we must have that $k\sim n$, so the first term in (\ref{log}) vanishes as $n\to \infty$. Since $k\sim n$, we can Taylor expand the logarithm in the middle term.
	\begin{equation}\label{remaminder}
	k\log\frac{k}{n} = k\log\left(1 + \frac{k-n}{n}\right) = k\left(\frac{k-n}{n} - \frac{1}{2}\left(\frac{k-n}{n}\right)^2 + O\left(\left(\frac{k-n}{n}\right)^3\right)\right).
	\end{equation}
	We have that $\frac{k-n}{\sqrt{n}} = x+o(1)$. From this it follows that
	\[
	k = n + \sqrt{n}(x+o(1)),\quad \frac{k-n}{n} = \frac{1}{\sqrt{n}}(x+o(1)). 
	\]
	Substituting these above gives
	\begin{align*}
		k\log \frac{k}{n} &= (n+\sqrt{n}(x+o(1)))\left(\frac{x+o(1)}{\sqrt{n}} - \frac{1}{2}\frac{(x+o(1))^2}{n} + O(n^{-3/2})\right)\\
		&= \sqrt{n}(x+o(1)) +\frac{1}{2}(x+o(1))^2 - \frac{1}{2}\frac{(x+o(1))^3}{\sqrt{n}} + O(n^{-1/2}).
	\end{align*}
	Finally, plugging this all back into (\ref{log}) gives
	\[
	\log(\sqrt{2\pi n}\Prob[S_n=k]) \sim-\frac{1}{2}(x+o(1))^2 + O(n^{-1/2}),
	\]
	which limits to $\exp(-x^2/2)$ as desired.
\end{proof}

\noindent\textbf{Problem 2. }
Find an example of random variables $X_n$ with densities $f_n$ so that $X_n$ converges weakly to the uniform distribution on $[0, 1]$ but $f_n(x)$ dos not converge to 1 for any $x\in [0,1]$.
\begin{solution}
	Let $f_{n,k}(x)$ be a typewriter sequence weighted to have integral 1, that is
	\[
	f_{n,k}(x) = \frac{1}{1-2^{-n}}\ind_{[0,1]\setminus [k\cdot 2^{-n},\ (k+1)\cdot 2^{-n}]}(x),\quad n = 1, 2, \ldots,\ k = 0, 1, \ldots, 2^{n}-1.
	\]
	Intuitively, $f_{n,k}$ is a flat line of height $\frac{1}{1-2^{-n}}$ over $[0,1]$ except for a gap at $[k\cdot 2^{-n}, (k+1)\cdot 2^{-n}]$, where it is zero. Since this gap slides along the unit interval indefinitely, $f_{n,k}$ does not converge pointwise anywhere. Now for any $\varphi$ bounded and continuous we have
	\[
	|\E_{n,k}[\varphi] - \E[\varphi]| = \int_0^1\varphi(x)(f_{n,k}(x) - 1)dx \leq \|\varphi\|_\infty\cdot 2^{-n}\to 0,
	\]
	so $X_n$ converges weakly to the uniform distribution on $[0,1]$.
\end{solution}

\noindent\textbf{Problem 3. }
Let $X_i$ be i.i.d. random variables each having exponential distribution with mean 1, and consider $M_n = \max_{i\leq n}X_i$. Show that $M_n - \log n$ converges weakly to the standard Gumbel distribution, i.e. the distribution with cumulative distribution function $F(x) = \exp(-e^{-x})$.
\begin{proof}
	For any $n$ and $t$ we have
	\[
	\Prob[M_n - \log n \leq t] = \Prob[M_n \leq \log n + t] = \Prob[X_i\leq \log n + t,\ 1\leq i\leq n].
	\]
	Since the $X_i$ are i.i.d. we can split this into a product
	\begin{align*}
	\Prob[M_n-\log n\leq t] &= \left(\Prob[X_i \leq \log_n + t]\right)^n\\
	&= \left(1 - \exp(-\log n  -t)\right)^n\\
	&= \left(1 - \frac{e^{-t}}{n}\right)^n\\
	&\to \exp(-e^{-t}),\quad \text{as }n\to \infty.
	\end{align*}
\end{proof}

\noindent\textbf{Problem 4. }Let $X_n$ be random variables and $c$ be a constant. Prove that weak convergence of $X_n$ to $c$ is equivalent to convergence of $X_n$ to $c$ in probability.
\begin{proof}
	Convergence in probability always implies weak convergence to the same limit, so we only need to show that weak convergence to $c$ implies convergence to $c$ in probability. The distribution function $F(t)$ of the random variable that is constantly $c$ is the shifted Heaviside function $F(t) = H(t-c)$. The only point of discontinuity of $F$ is at $t= a$. By the definition of weak convergence, if $F_n$ is the distribution function of $X_n$, we have that $F_n(x)\to F(x)$ for all $x\neq c$. For any $\epsilon>0$ we have
	\[
	\Prob[|X_n - c|<\epsilon] = F_n(c+\epsilon)-F_n(c-\epsilon)\to F(c+\epsilon) - F(c-\epsilon) = 1,
	\]
	so $X_n\to c$ in probability.
\end{proof}

\noindent\textbf{Problem 5. }
Consider the following statement:\\
\centerline{if $X_n\to X$ weakly and $Y_n\to Y$ weakly then $X_n+Y_n\to X+Y$ weakly.}
\begin{enumerate}[(a)]
	\item Find an example showing that this statement if false in general.
	\begin{solution}
		(I talked to Thomas Beardsley and Xiaowen Zhu about this one.) Let $X_n$ be a sequence of i.i.d. $\mcal{N}(0, 1)$ random variables. Set $Y_{2k} = X_{2k}$, but set $Y_{2k+1} = -X_{2k+1}$. Since the sequence of $X_n$'s are i.i.d., $X_n$ converges weakly to a standard normal random variable. Similarly, since the negative of a standard normal is a standard normal, $Y_n$ also converges weakly to a standard normal. However, the sequence $X_n + Y_n$ doesn't converge weakly at all since every odd term is identically zero and every even term has distribution $\mcal{N}(0, 4)$.
	\end{solution}

	\item Prove that if $Y$ is a constant, then the statement is true.
	\begin{proof}
		We'll show pointwise convergence of the characteristic functions.
		\begin{equation}\label{charf}
		\begin{split}
			\left|\E\left[e^{it(X_n+Y_n)} - e^{it(X+c)}\right]\right| &= \left|\E\left[e^{it(X_n+Y_n)} - e^{it(X_n+c)}+e^{it(X_n+c)}-e^{it(X+c)}\right]\right|\\
			& \leq \E\left|e^{itY_n} - e^{itc}\right| + e^{itc}\E\left[e^{itX_n} - e^{itX}\right].
		\end{split}
		\end{equation}
		Let's look at the first term. We split the region of integration like so
		\[
		\E\left|e^{itY_n} - e^{itc}\right| \cdot \ind_{|Y_n-c|\geq \epsilon} + \E\left|e^{itY_n} - e^{itc}\right| \cdot \ind_{|Y_n-c|< \epsilon}.
		\]
		Since weak convergence to a constant is equivalent to convergence in probability to a constant, the measure of the first region of integration goes to zero as $n\to \infty$, so the first term vanishes. Continuity of the exponential makes the second term small in $\epsilon$. Consequently, the first term on the last line of (\ref{charf}) can be made smaller than $\epsilon$ for $n$ large. The second term goes to zero since $X_n\to X$ weakly. Since the characteristic functions converge pointwise, we have weak convergence.
	\end{proof}

	\item Prove that if $X_n$ and $Y_n$ are independent then the statement is true.
	\begin{proof}
		We compute the characteristic functions and use independence to split the product.
		\begin{align*}
			\E\left[e^{it(X_n+Y_n)}\right] &= \E\left[ e^{itX_n}\right]\cdot \E\left[e^{itY_n}\right]\\
			&\to \E\left[e^{itX}\right]\E\left[e^{itY}\right].
		\end{align*}
		Since the characteristic functions converge pointwise, we have weak convergence.
	\end{proof}
\end{enumerate}

\noindent\textbf{Problem 6. }
\begin{enumerate}[(a)]
	\item Prove the following implication: if $X_n\to X$ weakly, $Y_n\geq 0$ and $Y_n\to c$ weakly where $c$ is a constant, then $X_nY_n\to cX$.
	\begin{proof}
		I think the idea is to use $Y_n\geq 0$ to say
		\[
		\Prob[X_nY_n \leq t] = \Prob[X_n \leq t/Y_n].
		\]
		For any $\epsilon$ we can split this probability:
		\[
		\Prob[X_nY_n\leq t] = \Prob[X_n \leq t/Y_n,\ |Y_n-c|<\epsilon] + \Prob[X_n \leq t/Y_n,\ |Y_n-c|\geq \epsilon].
		\]
		Since $Y_n$ converges weakly to $c$, it converges to $c$ in probability as well, so the second term goes to zero as $n\to \infty$. Informally, I want to say that $t/Y_n$ will become close to $t/c$.
	\end{proof}

	\item Let $Z_n$ be a random vector uniformly distributed on the unit Euclidean sphere of radius $\sqrt{n}$ in $\reals^n$. Prove that the distribution of the first coordinate of $Z_n$ converges weakly to the standard normal distribution.
	\begin{proof}
		Let $X_n$ be a standard normal random vector and let $Z_n = X_n\cdot \sqrt{n}/\|X_n\|_2$. $X_n/\|X_n\|_2$ has norm 1, so $Z_n$ is definitely valued on the $\sqrt{n}$-sphere. Since $X_n$ is rotation invariant in distribution, so is $Z_n$. 
	\end{proof}
\end{enumerate}

\end{document}