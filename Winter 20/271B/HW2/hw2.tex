\documentclass[11pt,letterpaper]{report}
\usepackage{amssymb,amsfonts,color,graphicx,amsmath,enumerate}
\usepackage{tikz}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{hyperref}

\newcommand{\naturals}{\mathbb{N}}
\newcommand{\integers}{\mathbb{Z}}
\newcommand{\complex}{\mathbb{C}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\rationals}{\mathbb{Q}}
\newcommand{\Lp}[2]{\left\|{#1}\right\|_{L^{#2}}}
\newcommand{\field}{\mathbb{F}}
\newcommand{\affine}{\mathbb{A}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\Var}{\text{Var}}
\newcommand{\ind}{\mathbbm{1}}
\newcommand{\Cov}{\text{Cov}}

\newenvironment{solution}
{\begin{proof}[Solution]}
{\end{proof}}

\voffset=-3cm
\hoffset=-2.25cm
\textheight=24cm
\textwidth=17.25cm
\addtolength{\jot}{8pt}
\linespread{1.3}

\begin{document}
\noindent{\em Liam Hardiman\hfill{February 20, 2020} }
\begin{center}
{\bf \Large 271B - Homework 2}
\vspace{0.2cm}
\hrule
\end{center}

\noindent\textbf{Problem 1. }
Let $S$, $T$, and $T_n$, $n = 1, 2, \ldots$ be stopping times (with respect to some filtration $\{\mcal{F}_t\}_{t\geq 0}$). Show that $T\lor S$, $T\land S$, $T+S$, $\sup_n T_n$ are also stopping times.
\begin{proof}
	The pointwise minimum, maximum, sum, and supremum of measurable functions are measurable. For the minimum and maximum we have
	\begin{align*}
		\{(T\land S) \leq t\} &= \{T\leq t\} \cup \{S\leq t\}\\
		\{(T\lor S) \leq t\} &= \{T\leq t\} \cap \{S\leq t\}.
	\end{align*}
	Unions and intersections of measurable sets are measurable, so both of these sets live in $\mcal{F}_t$. Thus, $T\land S$ and $T\lor S$ are stopping times. For the sum, we can write the set $\{T+S \leq t\}$ as a countable union:
	\[
	\{T+S \leq t\} = \bigcup_{\alpha, \beta \in \rationals,\ \alpha+\beta \leq t}\{T\leq \alpha\} \cap \{S\leq \beta\}.
	\]
	As $\mcal{F}_t$-measurability is closed under countable union and intersection, the sum is a stopping time. Finally, we have
	\[
	\{\sup_n T_n \leq t\} = \bigcap_{n=1}^\infty \{T_n \leq t\},
	\]
	which is measurable, so the supremum is also a stopping time.
\end{proof}

\noindent\textbf{Problem 2. }
Let $X_t$ be an adapted and continuous stochastic process, and define
\[
T_\Gamma = \inf\{t\geq 0: X_t\in \Gamma\}
\]
for $\Gamma$ a closed set. Show that $T_\Gamma$ is a stopping time.
\begin{proof}
	As $\Gamma$ is closed, for every $x$ there is a well-defined ``distance to $\Gamma$'' function
	\[
	d(x, \Gamma) = \inf_{y\in \Gamma}|x-y|.
	\]
	In fact, this function is continuous. Since $X_t$ has continuous paths and is $\mcal{F}_t$ measurable, the composition $Y_t = d(X_t, \Gamma)$ is $\mcal{F}_t$ measurable.\\

	\noindent Since $\Gamma$ is closed, $X_t\in \Gamma$ if and only if $Y_t = d(X_t, \Gamma) = 0$. From this it follows that $T_\Gamma>t$ if and only if $Y_s > 0$ for all $s\leq t$. Intuitively, if $T_\Gamma>t$, then $X_t$ arrives in $\Gamma$ at some time strictly later than $t$. In order for this to happen, $X_t$ must be outside of $\Gamma$ at all times $s\leq t$, in which case $Y_s = d(X_s, \Gamma)>0$. This set is ostensibly an uncountable intersection, but we can write it as a union of countable intersections by approximating by rational points.
	\[
	\{T_\Gamma > t\} = \bigcap_{s\leq t}\{Y_s>0\} = \bigcup_{n\geq 1}\bigcap_{q\in \rationals \cap [0, t]}\{Y_q  >1/n\} \in \mcal{F}_t.
	\]
	Hence, $T_\Gamma$ is a stopping time.
\end{proof}

\noindent\textbf{Problem 3. }
Show that if $X_t$ is a martingale with respect to some filtration (say $\mcal{F}_t$) then it is also a martingale with respect to the filtration generated by itself.
\begin{proof}
	Let $\mcal{G}_t = \sigma(X_s: s\leq t)$ be the filtration $X$ generates. We then have $\mcal{G}_t \subseteq \mcal{F}_t$ for all $t$ since $\mcal{G}_t$ is the smallest $\sigma$-algebra with respect to which $X_t$ is measurable. By the law of total expectation and the martingale property of $X_t$ with respect to $\mcal{F}_t$ we have for any $s\leq t$
	\[
	\E[X_t\ |\ \mcal{G}_s] = \E[\E[X_t\ |\ \mcal{F}_s]\ |\ \mcal{G}_s] = \E[X_s\ |\ \mcal{G}_s] = X_s.
	\]
	Thus, $X_t$ is a martingale with respect to $\{\mcal{G}_t\}$.
\end{proof}


\noindent\textbf{Problem 4. }
Let $a,b$ be deterministic and $f,g$ of class I. Show that if
\begin{equation}\label{ints}
a + \int_0^Tf_s\ dB_s = b+ \int_0^Tg_s\ dB_s
\end{equation}
then $a = b$ and $f = g$ a.a. for $(t, \omega) \in (0, T)\times \Omega$.
\begin{proof}
	Since $f$ and $g$ are of class I, $\int_0^t f_s\ dB_s$ and $\int_0^t g_s\ dB_s$ are martingales and $\int_0^0 f_s\ dB_s = 0$ a.s. (the same holds for $g$). Taking the expectation of both sides of the given relation shows that $a = b$ a.s. and
	\[
	\int_0^T(f_s - g_s)\ dB_s = 0.
	\]
	By the It\^o isometry we have
	\[
	0 = \E\left[\left(\int_0^T(f_s - g_s)\ dB)\right)^2\right] = \E\left[\int_0^T(f_s - g_s)^2\ ds\right].
	\]
	We conclude that $f_t(\omega) = g_t(\omega)$ for almost all $(t, \omega)\in (0, T)\times \Omega$.
\end{proof}

\noindent\textbf{Problem 5. }
Assume that $X_t$ is of class I and continuous in mean square on $[0, T]$, that is for $t\in [0, T]$
\[
\E[X_t^2]<\infty,\quad \lim_{s\to t}\E[(X_t-X_s)^2] = 0.
\]
Define
\[
\phi_t^{(n)} = \sum_jX_{t^{(n)}_{j-1}}\chi_{[t^{(n)}_{j-1}, t^{(n)}_j)}(t),\ t^{(n)}_j = j2^{-n}.
\]
Show that for $0\leq t\leq T$
\[
\int_0^tX_s\ dB_s = \lim_{n\to \infty}\int_0^t\phi^{(n)}_s\ dB_s,
\]
where the limit is in $L^2(\Prob)$.
\begin{proof}
	For any $n$ we have by the It\^o isometry
	\[
	\E\left[\left(\int_0^t(X_s - \phi^{(n)}_s)\ dB_s\right)^2\right] = \E\left[\int_0^t(X_s - \phi^{(n)}_s)^2\ ds\right] = \E\left[\sum_j\int_{t_{j-1}^{(n)}}^{t_j^{(n)}} (X_s - X_{t_{j-1}^{(n)}})^2\ ds \right].
	\]
	Now we claim that continuity in mean square on the compact set $[0, T]$ implies uniform continuity in mean square. Assuming this claim, we can choose $n$ large enough so that $\E[(X_s - X_{t_{j-1}^{(n)}})^2]$ is smaller than say $\epsilon$ for all $j$. For $n$ at least this large we have
	\[
	\E\left[\left(\int_0^t(X_s - \phi^{(n)}_s)\ dB_s\right)^2\right] \leq \sum_j(t_j^{(n)} - t_{j-1}^{(n)})\epsilon = \epsilon T.
	\]
	Since the $L^2$ distance between $\int_0^t\varphi_s^{(n)}\ dB_s$ and $\int_0^tX_s\ dB_s$ can be made arbitrarily small, we conclude that $\int_0^t \phi_s^{(n)}\ dB_s\to \int_0^tX_s\ dB_s$ in $L^2$.\\

	\noindent Now we show uniform mean square continuity. Suppose for the sake of contradiction that for some $\epsilon$ there is no $\delta$ such that $|s-t|<\delta$ implies that $\Lp{X_s - X_s}{2}<\epsilon$. Then we can find a sequence $s_n$, $t_n$ so that $|s_n-t_n|<1/n$ but $\Lp{X_s-X_t}{2}>\epsilon$. By the compactness of $[0, T]$, we can assume that $s_n\to s^*\in [0, T]$. We then have $\Lp{X_{s^*}-X_{t_n}}{2}>\epsilon$, but this contradicts the mean square continuity of $X$ at $s^*$.
\end{proof}

\noindent\textbf{Problem 6. }
Let $X_t$ be a deterministic continuous function and 
\[
Y_t = \int_0^tX_s\ dB_s.
\]
Deduce the law of the process $Y$.
\begin{solution}
	We assume $t\in [0, T]$ for some $T<\infty$. Since $X_t$ is continuous, it is bounded and $\int_0^tX_s\ ds<\infty$ for all $t$ and $\omega$. In particular, the family $\{X_t\}_{t\in [0, T]}$ is uniformly integrable in $\omega$, so we have
	\[
	\int_0^tX_s\ dB_s = \lim_{n\to \infty}\sum_{j=1}^{t/\Delta t}X_{t_{j-1}}\Delta B_{t_j},
	\]
	where $\Delta B_{t_j} = B_{t_j} - B_{t_{j-1}}$ and the limit is in $L^2$. (Alternatively, $X$ satisfies the hypotheses of problem 5, so we could have used the result from that problem to get this limit.) Since the Brownian increments on the right-hand side are disjoint, they are independent normal random variables, so the whole sum on the right is a normal random variable with distribution
	\[
	\mcal{N}\left(0, \sum_{j=1}^{t/\Delta t}X_{t_{j-1}}^2\Delta t\right).
	\]
	Since $X$ is continuous and deterministic, we recognize the above sum as a Riemann sum as $\Delta t\to 0$. Since the $L^2$ limit of normal random variables is normal when the mean and variance converge as sequences of real numbers, we have
	\[
	Y_t = \int_0^tX_s\ dB_s \sim \mcal{N}\left(0, \int_0^tX_s^2\ ds\right).
	\]
	Now the process $Y$ is Gaussian if and only if for every $t_1<\cdots<t_k \in T$, any linear combination of the $Y_{t_j}$'s has univariate normal distribution. Since $\int_a^b X_s\ dB_s = \int_a^c X_s\ dB_s + \int_c^bX_s\ dB_s$ for any $a<c<b$, we have
	\[
	c_1Y_{t_1} + c_2Y_{t_2} + \cdots + c_kY_{t_k} = (c_1 + \cdots + c_k)Y_{t_1} + (c_2 + \cdots + c_k)(Y_{t_2}-Y_{t_1}) + \cdots + c_k(Y_{t_k}-Y_{t_{k-1}}).
	\]
	The variables $Y_{t_j} - Y_{t_{j-1}}$ are themselves It\^o integrals over disjoint intervals, so they are independent normal random variables. We conclude that the above linear combination is normally distributed, so the process $Y$ is Gaussian.\\

	\noindent A Gaussian process, and therefore its law, is determined by its mean and covariance. Since $Y_t \sim \mcal{N}(0, \int_0^t X_s^2\ ds)$, the process $Y$ has zero mean. As for the covariance, we have for any $s,t$
	\begin{align*}
		\Cov(Y_s, Y_t) &= \E\left[\int_0^sX_u\ dB_u\cdot \int_0^tX_u\ dB_u \right]\\
		&= \E\left[\left(\int_0^{s\land t}X_u\ dB_u\right)^2\right] + \E\left[\int_0^{s\land t}X_u\ dB_u\cdot \int_{s\land t}^{s\lor t}X_u\ dB_u\right]\\
		&= \E\left[\int_0^{s\land t}X_u^2\ du\right] = \int_0^{s\land t}X_u^2\ du.
	\end{align*}
	The last line follows from the It\^o isometry and the independence of It\^o integrals over disjoint intervals.
\end{solution}

\end{document}