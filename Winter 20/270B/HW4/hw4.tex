\documentclass[11pt,letterpaper]{report}
\usepackage{amssymb,amsfonts,color,graphicx,amsmath,enumerate}
\usepackage{tikz}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{hyperref}

\newcommand{\naturals}{\mathbb{N}}
\newcommand{\integers}{\mathbb{Z}}
\newcommand{\complex}{\mathbb{C}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\rationals}{\mathbb{Q}}
\newcommand{\Lp}[2]{\left\|{#1}\right\|_{L^{#2}}}
\newcommand{\field}{\mathbb{F}}
\newcommand{\affine}{\mathbb{A}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\Var}{\text{Var}}
\newcommand{\ind}{\mathbbm{1}}
\newcommand{\Cov}{\text{Cov}}

\newenvironment{solution}
{\begin{proof}[Solution]}
{\end{proof}}

\voffset=-3cm
\hoffset=-2.25cm
\textheight=24cm
\textwidth=17.25cm
\addtolength{\jot}{8pt}
\linespread{1.3}

\begin{document}
\noindent{\em Liam Hardiman\hfill{March 18, 2020} }
\begin{center}
{\bf \Large 270B - Homework 4}
\vspace{0.2cm}
\hrule
\end{center}

\noindent\textbf{Problem 1. }
Let $(X_n)$ be an irreducible recurrent Markov chain with doubly-infinite transition matrix $P$. Let $\psi: \naturals\to \naturals$ be a bounded function satisfying
\[
\sum_{j=1}^\infty P_{ij}\psi(j) = \psi(i)\quad\text{for all }i\in \naturals.
\]
Show that $\psi$ is a constant function.
\begin{proof}
	First we claim that $\psi(X_n)$ is a martingale. Let $\mcal{F}_n$ be the filtration generated by $X_1, \ldots, X_n$. We then have by hypothesis
	\[
	\E[\psi(X_{n+1})|\mcal{F}_n] = \sum_{j}P_{X_n, j}\psi(j) = \psi(X_n),
	\]
	so $\psi(X_n)$ is a martingale with respect to this filtration. We showed on a previous homework assignment that bounded martingales converge almost surely, so $\psi(X_n)\to s$ almost surely for some $s\in \naturals$.\\

	\noindent Suppose $\psi$ is non-constant, so $\psi(i) = u$ and $\psi(j) = v$ for some $i\neq j$, $u\neq v$. Intuitively, since the Markov chain is irreducible and recurrent, it should return to states $i$ and $j$ infinitely often with probability 1. But since $\psi(i)\neq \psi(j)$, this means that $\psi(X_n)$ cannot converge almost surely. 
\end{proof}

\noindent\textbf{Problem 2. }
Let $S$ and $T$ be stopping times with respect to a filtration $(\mcal{F}_n)$. Denote by $(\mcal{F}_T)$ the collection of events $F$ such that $F\cap \{T\leq n\} \in \mcal{F}_n$ for all $n$.
\begin{enumerate}[(a)]
	\item Show that $\mcal{F}_T$ is a $\sigma$-algebra.
	\begin{proof}
		That $\emptyset$ and $\Omega$ are in $\mcal{F}_T$ immediately follows from $T$ being a stopping time. To show closure under complementation, write $F\in \mcal{F}_T$ like so
		\[
		F = (F\cap \{T\leq n\})\cup (F\cap \{T>n\}).
		\]
		This gives
		\[
		F^c\cap \{T\leq n\} = (F\cap \{T\leq n\})^c\cap (F\cap \{T>n\})^c\cap \{T\leq n\}.
		\]
		Since $F\cap \{T\leq n\}$ is in $\mcal{F}_n$, the above set is also in $\mcal{F}_n$.\\

		\noindent As for countable unions, let $F_1, F_2, \ldots \in \mcal{F}_T$. Then
		\[
		\left(\bigcup_{k=1}^\infty F_k\right)\cap \{T\leq n\} = \bigcup_{k=1}^\infty (F_k\cap \{T\leq n\}).
		\]
		Since $F_k\cap \{T\leq n\}\in \mcal{F}_n$ for all $n$, the above set is in $\mcal{F}_n$.
	\end{proof}

	\item Show that $T$ is measurable with respect to $\mcal{F}_T$.
	\begin{proof}
		$T$ is measurable with respect to $\mcal{F}_T$ if and only if $\{T\leq n\}\in \mcal{F}_T$ for all $n$. This follows immediately from the fact that $T$ is a stopping time with respect to the filtration $\mcal{F}_n$. 
	\end{proof}

	\item If $E\in \mcal{F}_S$, show that $E\cap \{S\leq T\}\in \mcal{F}_T$.
	\begin{proof}
		The idea is to write $\{S\leq T\}$ as $\cup_{k=1}^\infty \{T = k\}\cap \{S\leq k\}$. For any $E\in \mcal{F}_S$ we then have
		\begin{align*}
		(E\cap \{S\leq T\})\cap \{T\leq n\} &= \bigcup_{k=1}^\infty( E\cap \{S\leq k\}\cap \{T = k\}\cap\{T\leq n\} )\\
		&= \bigcup_{k=1}^n(E\cap \{S\leq k\}\cap \{T = k\}).
		\end{align*}
		Since $E\in \mcal{F}_S$, $E\cap \{S\leq k\}\in \mcal{F}_k$ for all $k$. Since $T$ is a stopping time with respect to $\mcal{F}_n$, $\{T = k\}\in \mcal{F}_k$ for all $k$. Consequently, the above union is in $\mcal{F}_n$, so $E\cap \{S\leq T\}\in \mcal{F}_T$.
	\end{proof}

	\item Show that if $S\leq T$ a.s. then $\mcal{F}_S\subset \mcal{F}_T$.
	\begin{proof}
		If $S\leq T$ a.s., then $\{S\leq T\}$ is a set with probability 1. By part (c), if $E\in \mcal{F}_S$, then all but a measure zero subset of $E$ is in $\mcal{F}_T$.
	\end{proof}
\end{enumerate}

\noindent\textbf{Problem 3. }
Let $(X_n)$ be a uniformly bounded [integrable, right?] martingale with respect to the filtration $(\mcal{F}_n)$. Let $S$ and $T$ be two stopping times satisfying $S\leq T$ a.s. Prove that
\[
X_T = \E[X|\mcal{F}_T]\quad\text{and}\quad X_S = \E[X_T|\mcal{F}_S]
\]
where $X$ is the almost sure limit of $X_n$.
\begin{proof}
	Since $X_n$ is uniformly integrable, we can reconstruct $X_n$ from its a.s. (and $L^1$) limit:
	\[
	X_n = \E[X|\mcal{F}_n].
	\]
	Now for any $F\in \mcal{F}_T$ we have $F\cap \{T=n\} \in \mcal{F}_n$ for all $n$.
	\[
	\int_FX_T\ d\Prob = \sum_{n=0}^\infty\int_{F\cap \{T=n\}}X_T\ d\Prob = \sum_{n=0}^\infty \int_{F\cap \{T = n\}}X_n\ d\Prob.
	\]
	By the definition of conditional expectation, the last integral above is equal to
	\[
	\sum_{n=1}^\infty\int_{F\cap \{T = n\}}X\ d\Prob = \int_FX\ d\Prob.
	\]
	Again, by the definition of conditional expectation, we have $X_T = \E[X|\mcal{F}_T]$.\\

	\noindent By problem 2(d), we have that since $S\leq T$ a.s., $\mcal{F}_S\subset \mcal{F}_T$. Consequently, we have by the law of total expectation
	\[
	\E[X_T|\mcal{F}_S] = \E[\E[X|\mcal{F}_T]\ |\ \mcal{F}_S] = \E[X|\mcal{F}_S] = X_S.
	\]
\end{proof}

\noindent\textbf{Problem 4. }
A die is rolled repeatedly. Which of the following are Markov chains? For those that are, compute the transition matrix.
\begin{enumerate}[(a)]
	\item The largest number $X_n$ shown up to the $n$-th roll.
	\begin{solution}
		Intuitively, this should be a Markov chain: to check if the current roll is the largest thus far, we need only compare it to the largest roll seen before. More concretely, consider $i_1\leq i_2 \leq \cdots \leq i_n$. Then
		\[
		\Prob[X_{n+1} = i_{n+1}\ |\ X_n = i_n, \ldots, X_1 = i_1] = \begin{cases}
			\frac{1}{6}&\text{if }i_{n+1}\geq i_n\\
			0&\text{otherwise}
		\end{cases} = \Prob[X_{n+1} = i_{n+1}\ |\ X_n = i_n].
		\]
	\end{solution}

	\item The number $N_n$ of sixes in $n$ rolls.
	\begin{solution}
		Again, intuition tells us that this should be a Markov chain: the number of sixes on the $n+1$-st roll will either be the same as or one greater than the number of sixes on the $n$-th roll. 
	\end{solution}
\end{enumerate}

\end{document}